{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nThis is the documentation for the \ncfgov-refresh\n project, a redesign of the \nwww.consumerfinance.gov\n website. It is organized thematically in order to create a central repository for all information pertaining to cfgov-refresh.\n\n\nDisclaimer\n\n\nThis project is a work in progress.\n Nothing presented in this repo\u2014whether in the source code, issue tracker, or wiki\u2014is a final product unless it is marked as such or appears on \nwww.consumerfinance.gov\n. In-progress updates may appear on \nbeta.consumerfinance.gov\n.\n\n\nTechnology stack\n\n\nThe standard technology stack for development of cfgov-refresh within the CFPB consists of the following base:\n\n\n\n\nMac OSX\n\n\nHomebrew\n - package manager for installing system software on OSX\n\n\nPython and PIP (Python package manager)\n\n\nWordPress API data source URL\n\n\nJinja templates\n for front-end rendering\n\n\nWagtail CMS\n for content administration\n\n\nDependencies, listed below\n\n\n\n\nDependencies\n\n\n\n\nElasticsearch\n:\n  Used for full-text search capabilities and content indexing.\n\n\nNode\n and \nnpm (Node Package Manager)\n:\n  Used for downloading and managing front-end dependencies and assets.\n\n\n\n\nFor Vagrant Virtualbox usage (\n The Vagrant box is not currently working.)\n\n\n\n\nVirtualBox\n\n\nVagrant\n\n\npython \n=  2.6\n\n\nansible \n= 1.9", 
            "title": "Introduction"
        }, 
        {
            "location": "/#introduction", 
            "text": "This is the documentation for the  cfgov-refresh  project, a redesign of the  www.consumerfinance.gov  website. It is organized thematically in order to create a central repository for all information pertaining to cfgov-refresh.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#disclaimer", 
            "text": "This project is a work in progress.  Nothing presented in this repo\u2014whether in the source code, issue tracker, or wiki\u2014is a final product unless it is marked as such or appears on  www.consumerfinance.gov . In-progress updates may appear on  beta.consumerfinance.gov .", 
            "title": "Disclaimer"
        }, 
        {
            "location": "/#technology-stack", 
            "text": "The standard technology stack for development of cfgov-refresh within the CFPB consists of the following base:   Mac OSX  Homebrew  - package manager for installing system software on OSX  Python and PIP (Python package manager)  WordPress API data source URL  Jinja templates  for front-end rendering  Wagtail CMS  for content administration  Dependencies, listed below", 
            "title": "Technology stack"
        }, 
        {
            "location": "/#dependencies", 
            "text": "Elasticsearch :\n  Used for full-text search capabilities and content indexing.  Node  and  npm (Node Package Manager) :\n  Used for downloading and managing front-end dependencies and assets.   For Vagrant Virtualbox usage (  The Vagrant box is not currently working.)   VirtualBox  Vagrant  python  =  2.6  ansible  = 1.9", 
            "title": "Dependencies"
        }, 
        {
            "location": "/installation/", 
            "text": "Installation and configuration for cfgov-refresh\n\n\nClone the repository\n\n\nUsing the console, navigate to the root directory in which your projects\nlive and clone this project's repository:\n\n\ngit clone git@github.com:cfpb/cfgov-refresh.git\ncd cfgov-refresh\n\n\n\nYou may also wish to fork the repository on GitHub and clone the resultant\npersonal fork. This is advised if you are going to be doing development on\n\ncfgov-refresh\n and contributing to the project.\n\n\nThere are two ways to install cfgov-refresh:\n\n\n\n\nStand-alone installation\n\n\nDocker-compose installation\n\n\n\n\nStand-alone installation\n\n\nThese instructions are somewhat specific to developing on Mac OS X,\nbut if you're familiar with other Unix-based systems,\nit should be fairly easy to adapt them to your needs.\n\n\nInstall system-level requirements\n\n\nvirtualenv \n virtualenvwrapper Python modules\n\n\nInstall \nvirtualenv\n\nand \nvirtualenvwrapper\n\nto be able to create a local environment for your server:\n\n\npip install virtualenv virtualenvwrapper\n\n\n\nAutoenv module\n\n\nThis project uses a large number of environment variables.\n\n\nTo automatically define environment variables and launch the virtualenv\nupon \ncd\ning to the project folder,\n\ninstall Autoenv\n.\nWe recommend using \nHomebrew\n:\n\n\nbrew install autoenv\n\n\n\nAfter installation, Homebrew will output instructions similar to:\n\n\nTo finish the installation, source activate.sh in your shell:\n  source /Users/[YOUR USERNAME]/homebrew/opt/autoenv/activate.sh\n\n\n\nRun that now for your initial setup.\nAny time you run the project you\u2019ll need to run that last line, so\nif you\u2019ll be working with the project consistently,\nwe suggest adding it to your Bash profile by running:\n\n\necho 'source /Users/[YOUR USERNAME]/homebrew/opt/autoenv/activate.sh' \n ~/.bash_profile\n\n\n\nIf you need to find this info again later, you can run:\n\n\nbrew info autoenv\n\n\n\n\n\nNote\n\n\nIf you use Zsh you\u2019ll need to use\n\nzsh-autoenv\n,\nbut we can\u2019t provide support for issues that may arise.\n\n\n\n\nMySQL\n\n\nIf you're developing on OS X, this should be installed by default,\nand you shouldn't have to do anything else to get it working.\nYou can optionally install a different version with Homebrew.\n\n\nElasticsearch\n\n\n\n\nWarning\n\n\nThese instructions are deprecated since Elasticsearch 1.7\nis no longer supported by \nbrew\n.\n\n\n\n\nInstall Elasticsearch 1.7\n\nhowever you\u2019d like. We use \nHomebrew\n for developing on OS X):\n\n\nbrew tap homebrew/versions\nbrew search elasticsearch\nbrew install homebrew/versions/elasticsearch17\n\n\n\nJust as with Autoenv, Homebrew will output similar instructions after installation:\n\n\n# To have launchd start homebrew/versions/elasticsearch17 now and restart at login:\n  brew services start homebrew/versions/elasticsearch17\n# Or, if you don't want/need a background service you can just run:\n  elasticsearch --config=/Users/[YOUR USERNAME]/homebrew/opt/elasticsearch17/config/elasticsearch.yml\n\n\n\nAny time you resume work on the project after restarting your machine,\nyou\u2019ll need to open a new tab and run that last line.\nIf you\u2019ll be working on the project consistently,\nwe suggest using the first option, so you don't have to worry about that.\nNote that some older versions of Homebrew may suggest\nusing \nlaunchctl\n instead of \nbrew services\n.\n\n\nIf you need to find this info again later, you can run:\n\n\nbrew info elasticsearch17\n\n\n\nFront-end dependencies\n\n\nThe cfgov-refresh front end currently uses the following frameworks / tools:\n\n\n\n\nGulp\n: task management for pulling in assets,\n  linting and concatenating code, etc.\n\n\nLess\n: CSS pre-processor.\n\n\nCapital Framework\n:\n  User interface pattern-library produced by the CFPB.\n\n\n\n\n\n\nNote\n\n\nIf you\u2019re new to Capital Framework, we encourage you to\n\nstart here\n.\n\n\n\n\n\n\n\n\nInstall \nNode.js\n however you\u2019d like.\n   We recommend using \nnvm\n, though.\n\n\n\n\n\n\nInstall \nGulp\n:\n\n\n\n\n\n\nnpm install -g gulp\n\n\n\n\n\nNote\n\n\nThis project requires Node.js v8 or higher, and npm v5 or higher.\n\n\n\n\nWebfonts\n\n\nThe site uses a proprietary licensed font, Avenir.\nIf you want to pull this from a content delivery network (CDN),\nyou can set the\n\n@use-font-cdn\n\nto \ntrue\n and rebuild the assets with \ngulp build\n.\nIf you want to install self-hosted fonts locally, you can place the font files\nin \nstatic.in/cfgov-fonts/fonts/\n and restart the local web server.\nIf you are a CFPB employee, you can perform this step with:\n\n\ncd static.in/ \n git clone https://[GHE]/CFGOV/cfgov-fonts/\n\nWhere \n[GHE]\n is our GitHub Enterprise URL.\n\n\nSet up your environment\n\n\nIf this is the first time you're setting up the project, run the following\nscript to copy \n.env_SAMPLE\n to \n.env\n, export your environment variables,\nand activate your virtualenv for the first time.\n\n\nsource load-env.sh\n\n\n\nEach time you start a new session for working on this project, you'll need to\nget those environment variables and get your virtualenv running again.\nIf you setup Autoenv earlier, this will happen for you automatically when you\n\ncd\n into the project directory.\n\n\nIf you prefer not to use Autoenv, just be sure to \nsource .env\n every time\nyou start a new session of work on the project.\n\n\nRun the setup script\n\n\nAt this point, your machine should have everything it needs to automate the\nrest of the setup process.\n\n\nIf you haven't cloned this repo yet, clone it to a local folder.\nBecause related projects will need to be installed as siblings to this project,\nwe recommend putting them all in their own folder, e.g., \n~/Projects/cf.gov\n.\n\n\nOnce cloned, from the project root (\n~/Projects/cf.gov/cfgov-refresh/\n),\nrun this command to complete the setup process:\n\n\nsource setup.sh\n\n\n\nThis will take several minutes, going through the steps in these scripts:\n\n\n\n\nfrontend.sh\n\n\nbackend.sh\n\n\n\n\nOnce complete, you should have a fully functioning local environment,\nready for you to develop against!\n\n\nThere are some \noptional setup steps\n\nthat you may want to perform before continuing.\n\n\nWant to know more about what the setup scripts are doing?\n\nRead the detailed rundown.\n\n\nGet any errors? \nSee our troubleshooting tips.\n\n\nContinue following the \nusage instructions\n.\n\n\nDocker-compose installation\n\n\nTools we use for developing with Docker\n\n\n\n\nDocker\n: You may not need to interact directly with Docker: but you\n  should know that it's a client/server application for managing \"containers\"\n  (a way of running software in an isolated environment) and \"images\" (a\n  snapshot of all of the files neccessary to run a container).\n\n\nDocker Compose\n: Compose allows you to configure and run a collection of\n  connected containers (like a web application and it's database)\n\n\nDocker Machine\n: Docker only runs natively on Linux and Windows. On OS X,\n  we'll use Docker Machine to start the Docker server in a virtual linux\n  environment (using Virtualbox)\n\n\n\n\n1. Setup your Docker environment\n\n\nIf you have never installed Docker before, follow the instructions\n\nhere\n or from your operating\nsystem vendor. If you are on a mac and are unable to install the official\n\"Docker for Mac\" package, the quickstart instructions below might help.\n\n\nIf you are on a machine that is already set up to run Linux docker containers,\nplease install \nDocker Compose\n.\nIf \ndocker-compose ps\n runs without error, you can can go to step 2.\n\n\nMac + Homebrew + Virtualbox quickstart\n\n\nStarting assumptions\n: You already have homebrew and virtualbox installed.\nYou can run \nbrew search docker\n without error.\n\n\nInstall Docker, Docker Machine, and Docker Compose:\n\nbrew install docker docker-compose docker-machine\n\n\nAt this point, \ndocker-compose ps\n should run without error.\n\n\n2. Setup your frontend environment\n\n\nRefer to the \nfront-end dependencies\n described above\nin the \nstandalone installation instructions\n.\n\n\n3. Run setup\n\n\n./setup.sh docker\n\n\nThis will install and build the frontend and set up the docker environment.\n\n\n4. Run the for the first time\n\n\n./runserver.sh docker\n\n\nThis will download and/or build images, and then start the containers, as\ndescribed in the docker-compose.yml file. This will take a few minutes, or\nlonger if you are on a slow internet connection.\n\n\nWhen it's all done, you should be able to load \nhttp://localhost:8000\n in your\nbrowser, and see a database error.\n\n\n3. Setup the database\n\n\nRun \n./shell.sh\n. This opens a bash shell inside your Python container.\n\n\nYou can either \nload initial data\n per the\ninstructions below, or load a database dump.\n\n\nYou could save some time and effort later (if you have access to the CFPB\nnetwork), by configuring a URL for database dumps in the \n.python_env\n file.\n\n\nCFGOV_PROD_DB_LOCATION=https://(rest of the URL)\n\n\n\nYou can get that URL at\n[GHE]/CFGOV/platform/wiki/Database-downloads#resources-available-via-s3\n\n\nWith \nCFGOV_PROD_DB_LOCATION\n in \n.python_env\n you should be able to run:\n\n\n./refresh-data.sh\n\n\nOtherwise, \nthe instructions to load a database dump\n\nbelow should be enough to get you started.\n\n\nOnce you have a database loaded, you should have a functioning copy of site\nworking at \nhttp://localhost:8000\n\n\n4. Next Steps\n\n\nSee the Docker section of the \nusage\n page to continue after that.\n\n\nOptional steps\n\n\nLoad initial data into database\n\n\nThe \ninitial-data.sh\n script can be used to initialize a new database to make\nit easy to get started working on Wagtail. This script first ensures that all\nmigrations are applied to the database, and then does the following:\n\n\n\n\nCreates an \nadmin\n superuser with a password as specified in the\n\nWAGTAIL_ADMIN_PW\n environment variable, if set.\n\n\nIf it doesn't already exist, creates a new Wagtail home page named \nCFGOV\n,\nwith a slug of \ncfgov\n.\n\n\nUpdates the default Wagtail site to use the port defined by the\n\nDJANGO_HTTP_PORT\n environment variable, if defined; otherwise this port is\nset to 80.\n\n\nIf it doesn't already exist, creates a new\n\nwagtail-sharing\n \nSharingSite\n with\na hostname and port defined by the \nDJANGO_STAGING_HOSTNAME\n and\n\nDJANGO_HTTP_PORT\n environment variables.\n\n\n\n\nLoad a database dump\n\n\nIf you're installing this fresh, the initial data you receive will not be\nas extensive as you'd probably like it to be.\n\n\nYou can get a database dump by:\n\n\n\n\nGoing to [GHE]/CFGOV/platform/wiki/Database-downloads\n\n\nSelecting one of the extractions and downloading the\n   \nproduction_django.sql.gz\n file\n\n\nUnzip it\n\n\nRun:\n\n\n\n\n./refresh-data.sh /path/to/dump.sql\n\n\n\nThe \nrefresh-data.sh\n script will apply the same changes as the\n\ninitial-data.sh\n script described above (including setting up the \nadmin\n\nsuperuser), but will not apply migrations.\n\n\nTo apply any unapplied migrations to a database created from a dump, run:\n\n\npython cfgov/manage.py migrate\n\n\n\nSet variables for working with the GovDelivery API\n\n\nUncomment and set the GovDelivery environment variables in your \n.env\n file.\n\n\n\n\nNote\n\n\nGovDelivery is a third-party web service that powers our emails.\nThe API is used by subscribe forms on our website.\nUsers may decide to swap this tool out for another third-party service.\n\n\n\n\nCurious about what the setup scripts are doing?\n\n\nHere's a rundown of each of the scripts called by \nsetup.sh\n and what they do.\n\n\n1. \nfrontend.sh\n\n\n\n\nInitialize project dependency directories\n (\ninit\n)\n\n\n\n\nThis script first checks for an argument passed from the command line\n   that can trigger different options for different environments.\n   Since you ran it with no arguments, it will set up the dev environment.\n\n\nIt then creates a checksum for \npackage-lock.json\n (if it exists) and\n   \npackage.json\n.\n   This will be used later to determine if dependencies need to be installed.\n\n\nIt will then set some env vars for the Node dependency directories.\n1. \nClean and install project dependencies\n (\nclean_and_install\n)\n\n\nThe script will now compare the checksums to see if it needs to install\n   dependencies, or if they are already up-to-date.\n\n\nIf the checksums do not match, the script will empty out all installed\n   dependencies (\nclean\n) so the new installation can start fresh,\n   then install the latest requested dependencies (\ninstall\n).\n\n\nThe \ndevDependencies\n from \npackage.json\n are not installed\n   if the environment is production, and if it's the dev or test environment,\n   it checks to see if Protractor is globally installed.\n\n\nFinally, it creates a new checksum for future comparisons.\n1. \nRun tasks to build the project for distribution\n (\nbuild\n)\n\n\nFinally, the script runs \ngulp build\n to rebuild the front-end assets.\n   It no longer cleans first, because the gulp-changed plugin prevents\n   rebuilding assets that haven't changed since the last build.\n\n\nIf this is the production environment, it also triggers style and script\n   builds for \nondemand\n and \nnemo\n, which aren't part of a standard\n   \ngulp build\n.\n\n\n2. \nbackend.sh\n\n\n\n\nNote\n\n\nbackend.sh\n is not used for our Docker setup.\n\n\n\n\n\n\nConfirm environment\n (\ninit\n)\n\n\n\n\nThis script first checks for an argument passed from the command line\n   that can trigger different options for different environments.\n   Since you ran it with no arguments, it will set up the dev environment.\n\n\nIt will then run a script to ensure that you're in a virtualenv.\n   If not, the script will end, to prevent you from accidentally installing\n   your Python dependencies globally.\n1. \nInstall project dependencies\n (\ninstall\n)\n\n\nPython dependencies are installed into the virtualenv via pip.\n   Dependencies vary slightly depending on whether we're in dev, test, or prod.\n1. \nSetup MySQL server\n (\ndb_setup\n)\n\n\nFinally, the script will start the MySQL server, if it's not already running,\n   run \ncreate-mysql-db.sh\n to create the database using\n   the variables given in \n.env\n, if it's not already there,\n   and will run \ninitial-data.sh\n to create the first Wagtail user\n   and load some basic initial data.\n\n\nTroubleshooting\n\n\nHere are some common issues and how you can fix them:\n\n\nErrors referencing South, or other Python errors:\n\n\nSince moving to Django 1.8, we use Django's built-in migration engine,\nand we no longer use South.\nIf you're getting South errors, you probably have it installed globally.\nTo solve this, from outside the virtual environment, run \npip uninstall south\n.\n\n\nIf you're getting other kinds of Python errors (for example, when running tox),\nyou may even want to go as far as uninstalling all globally-installed\nPython packages: \npip freeze | grep -v \"^-e\" | xargs pip uninstall -y\n.\nAfter doing that, you'll need to reinstall virtualenv:\n\npip install virtualenv virtualenvwrapper\n.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#installation-and-configuration-for-cfgov-refresh", 
            "text": "", 
            "title": "Installation and configuration for cfgov-refresh"
        }, 
        {
            "location": "/installation/#clone-the-repository", 
            "text": "Using the console, navigate to the root directory in which your projects\nlive and clone this project's repository:  git clone git@github.com:cfpb/cfgov-refresh.git\ncd cfgov-refresh  You may also wish to fork the repository on GitHub and clone the resultant\npersonal fork. This is advised if you are going to be doing development on cfgov-refresh  and contributing to the project.  There are two ways to install cfgov-refresh:   Stand-alone installation  Docker-compose installation", 
            "title": "Clone the repository"
        }, 
        {
            "location": "/installation/#stand-alone-installation", 
            "text": "These instructions are somewhat specific to developing on Mac OS X,\nbut if you're familiar with other Unix-based systems,\nit should be fairly easy to adapt them to your needs.", 
            "title": "Stand-alone installation"
        }, 
        {
            "location": "/installation/#install-system-level-requirements", 
            "text": "", 
            "title": "Install system-level requirements"
        }, 
        {
            "location": "/installation/#virtualenv-virtualenvwrapper-python-modules", 
            "text": "Install  virtualenv \nand  virtualenvwrapper \nto be able to create a local environment for your server:  pip install virtualenv virtualenvwrapper", 
            "title": "virtualenv &amp; virtualenvwrapper Python modules"
        }, 
        {
            "location": "/installation/#autoenv-module", 
            "text": "This project uses a large number of environment variables.  To automatically define environment variables and launch the virtualenv\nupon  cd ing to the project folder, install Autoenv .\nWe recommend using  Homebrew :  brew install autoenv  After installation, Homebrew will output instructions similar to:  To finish the installation, source activate.sh in your shell:\n  source /Users/[YOUR USERNAME]/homebrew/opt/autoenv/activate.sh  Run that now for your initial setup.\nAny time you run the project you\u2019ll need to run that last line, so\nif you\u2019ll be working with the project consistently,\nwe suggest adding it to your Bash profile by running:  echo 'source /Users/[YOUR USERNAME]/homebrew/opt/autoenv/activate.sh'   ~/.bash_profile  If you need to find this info again later, you can run:  brew info autoenv   Note  If you use Zsh you\u2019ll need to use zsh-autoenv ,\nbut we can\u2019t provide support for issues that may arise.", 
            "title": "Autoenv module"
        }, 
        {
            "location": "/installation/#mysql", 
            "text": "If you're developing on OS X, this should be installed by default,\nand you shouldn't have to do anything else to get it working.\nYou can optionally install a different version with Homebrew.", 
            "title": "MySQL"
        }, 
        {
            "location": "/installation/#elasticsearch", 
            "text": "Warning  These instructions are deprecated since Elasticsearch 1.7\nis no longer supported by  brew .   Install Elasticsearch 1.7 \nhowever you\u2019d like. We use  Homebrew  for developing on OS X):  brew tap homebrew/versions\nbrew search elasticsearch\nbrew install homebrew/versions/elasticsearch17  Just as with Autoenv, Homebrew will output similar instructions after installation:  # To have launchd start homebrew/versions/elasticsearch17 now and restart at login:\n  brew services start homebrew/versions/elasticsearch17\n# Or, if you don't want/need a background service you can just run:\n  elasticsearch --config=/Users/[YOUR USERNAME]/homebrew/opt/elasticsearch17/config/elasticsearch.yml  Any time you resume work on the project after restarting your machine,\nyou\u2019ll need to open a new tab and run that last line.\nIf you\u2019ll be working on the project consistently,\nwe suggest using the first option, so you don't have to worry about that.\nNote that some older versions of Homebrew may suggest\nusing  launchctl  instead of  brew services .  If you need to find this info again later, you can run:  brew info elasticsearch17", 
            "title": "Elasticsearch"
        }, 
        {
            "location": "/installation/#front-end-dependencies", 
            "text": "The cfgov-refresh front end currently uses the following frameworks / tools:   Gulp : task management for pulling in assets,\n  linting and concatenating code, etc.  Less : CSS pre-processor.  Capital Framework :\n  User interface pattern-library produced by the CFPB.    Note  If you\u2019re new to Capital Framework, we encourage you to start here .     Install  Node.js  however you\u2019d like.\n   We recommend using  nvm , though.    Install  Gulp :    npm install -g gulp   Note  This project requires Node.js v8 or higher, and npm v5 or higher.", 
            "title": "Front-end dependencies"
        }, 
        {
            "location": "/installation/#webfonts", 
            "text": "The site uses a proprietary licensed font, Avenir.\nIf you want to pull this from a content delivery network (CDN),\nyou can set the @use-font-cdn \nto  true  and rebuild the assets with  gulp build .\nIf you want to install self-hosted fonts locally, you can place the font files\nin  static.in/cfgov-fonts/fonts/  and restart the local web server.\nIf you are a CFPB employee, you can perform this step with:  cd static.in/   git clone https://[GHE]/CFGOV/cfgov-fonts/ \nWhere  [GHE]  is our GitHub Enterprise URL.", 
            "title": "Webfonts"
        }, 
        {
            "location": "/installation/#set-up-your-environment", 
            "text": "If this is the first time you're setting up the project, run the following\nscript to copy  .env_SAMPLE  to  .env , export your environment variables,\nand activate your virtualenv for the first time.  source load-env.sh  Each time you start a new session for working on this project, you'll need to\nget those environment variables and get your virtualenv running again.\nIf you setup Autoenv earlier, this will happen for you automatically when you cd  into the project directory.  If you prefer not to use Autoenv, just be sure to  source .env  every time\nyou start a new session of work on the project.", 
            "title": "Set up your environment"
        }, 
        {
            "location": "/installation/#run-the-setup-script", 
            "text": "At this point, your machine should have everything it needs to automate the\nrest of the setup process.  If you haven't cloned this repo yet, clone it to a local folder.\nBecause related projects will need to be installed as siblings to this project,\nwe recommend putting them all in their own folder, e.g.,  ~/Projects/cf.gov .  Once cloned, from the project root ( ~/Projects/cf.gov/cfgov-refresh/ ),\nrun this command to complete the setup process:  source setup.sh  This will take several minutes, going through the steps in these scripts:   frontend.sh  backend.sh   Once complete, you should have a fully functioning local environment,\nready for you to develop against!  There are some  optional setup steps \nthat you may want to perform before continuing.  Want to know more about what the setup scripts are doing? Read the detailed rundown.  Get any errors?  See our troubleshooting tips.  Continue following the  usage instructions .", 
            "title": "Run the setup script"
        }, 
        {
            "location": "/installation/#docker-compose-installation", 
            "text": "", 
            "title": "Docker-compose installation"
        }, 
        {
            "location": "/installation/#tools-we-use-for-developing-with-docker", 
            "text": "Docker : You may not need to interact directly with Docker: but you\n  should know that it's a client/server application for managing \"containers\"\n  (a way of running software in an isolated environment) and \"images\" (a\n  snapshot of all of the files neccessary to run a container).  Docker Compose : Compose allows you to configure and run a collection of\n  connected containers (like a web application and it's database)  Docker Machine : Docker only runs natively on Linux and Windows. On OS X,\n  we'll use Docker Machine to start the Docker server in a virtual linux\n  environment (using Virtualbox)", 
            "title": "Tools we use for developing with Docker"
        }, 
        {
            "location": "/installation/#1-setup-your-docker-environment", 
            "text": "If you have never installed Docker before, follow the instructions here  or from your operating\nsystem vendor. If you are on a mac and are unable to install the official\n\"Docker for Mac\" package, the quickstart instructions below might help.  If you are on a machine that is already set up to run Linux docker containers,\nplease install  Docker Compose .\nIf  docker-compose ps  runs without error, you can can go to step 2.", 
            "title": "1. Setup your Docker environment"
        }, 
        {
            "location": "/installation/#mac-homebrew-virtualbox-quickstart", 
            "text": "Starting assumptions : You already have homebrew and virtualbox installed.\nYou can run  brew search docker  without error.  Install Docker, Docker Machine, and Docker Compose: brew install docker docker-compose docker-machine  At this point,  docker-compose ps  should run without error.", 
            "title": "Mac + Homebrew + Virtualbox quickstart"
        }, 
        {
            "location": "/installation/#2-setup-your-frontend-environment", 
            "text": "Refer to the  front-end dependencies  described above\nin the  standalone installation instructions .", 
            "title": "2. Setup your frontend environment"
        }, 
        {
            "location": "/installation/#3-run-setup", 
            "text": "./setup.sh docker  This will install and build the frontend and set up the docker environment.", 
            "title": "3. Run setup"
        }, 
        {
            "location": "/installation/#4-run-the-for-the-first-time", 
            "text": "./runserver.sh docker  This will download and/or build images, and then start the containers, as\ndescribed in the docker-compose.yml file. This will take a few minutes, or\nlonger if you are on a slow internet connection.  When it's all done, you should be able to load  http://localhost:8000  in your\nbrowser, and see a database error.", 
            "title": "4. Run the for the first time"
        }, 
        {
            "location": "/installation/#3-setup-the-database", 
            "text": "Run  ./shell.sh . This opens a bash shell inside your Python container.  You can either  load initial data  per the\ninstructions below, or load a database dump.  You could save some time and effort later (if you have access to the CFPB\nnetwork), by configuring a URL for database dumps in the  .python_env  file.  CFGOV_PROD_DB_LOCATION=https://(rest of the URL)  You can get that URL at\n[GHE]/CFGOV/platform/wiki/Database-downloads#resources-available-via-s3  With  CFGOV_PROD_DB_LOCATION  in  .python_env  you should be able to run:  ./refresh-data.sh  Otherwise,  the instructions to load a database dump \nbelow should be enough to get you started.  Once you have a database loaded, you should have a functioning copy of site\nworking at  http://localhost:8000", 
            "title": "3. Setup the database"
        }, 
        {
            "location": "/installation/#4-next-steps", 
            "text": "See the Docker section of the  usage  page to continue after that.", 
            "title": "4. Next Steps"
        }, 
        {
            "location": "/installation/#optional-steps", 
            "text": "", 
            "title": "Optional steps"
        }, 
        {
            "location": "/installation/#load-initial-data-into-database", 
            "text": "The  initial-data.sh  script can be used to initialize a new database to make\nit easy to get started working on Wagtail. This script first ensures that all\nmigrations are applied to the database, and then does the following:   Creates an  admin  superuser with a password as specified in the WAGTAIL_ADMIN_PW  environment variable, if set.  If it doesn't already exist, creates a new Wagtail home page named  CFGOV ,\nwith a slug of  cfgov .  Updates the default Wagtail site to use the port defined by the DJANGO_HTTP_PORT  environment variable, if defined; otherwise this port is\nset to 80.  If it doesn't already exist, creates a new wagtail-sharing   SharingSite  with\na hostname and port defined by the  DJANGO_STAGING_HOSTNAME  and DJANGO_HTTP_PORT  environment variables.", 
            "title": "Load initial data into database"
        }, 
        {
            "location": "/installation/#load-a-database-dump", 
            "text": "If you're installing this fresh, the initial data you receive will not be\nas extensive as you'd probably like it to be.  You can get a database dump by:   Going to [GHE]/CFGOV/platform/wiki/Database-downloads  Selecting one of the extractions and downloading the\n    production_django.sql.gz  file  Unzip it  Run:   ./refresh-data.sh /path/to/dump.sql  The  refresh-data.sh  script will apply the same changes as the initial-data.sh  script described above (including setting up the  admin \nsuperuser), but will not apply migrations.  To apply any unapplied migrations to a database created from a dump, run:  python cfgov/manage.py migrate", 
            "title": "Load a database dump"
        }, 
        {
            "location": "/installation/#set-variables-for-working-with-the-govdelivery-api", 
            "text": "Uncomment and set the GovDelivery environment variables in your  .env  file.   Note  GovDelivery is a third-party web service that powers our emails.\nThe API is used by subscribe forms on our website.\nUsers may decide to swap this tool out for another third-party service.", 
            "title": "Set variables for working with the GovDelivery API"
        }, 
        {
            "location": "/installation/#curious-about-what-the-setup-scripts-are-doing", 
            "text": "Here's a rundown of each of the scripts called by  setup.sh  and what they do.", 
            "title": "Curious about what the setup scripts are doing?"
        }, 
        {
            "location": "/installation/#1-frontendsh", 
            "text": "Initialize project dependency directories  ( init )   This script first checks for an argument passed from the command line\n   that can trigger different options for different environments.\n   Since you ran it with no arguments, it will set up the dev environment.  It then creates a checksum for  package-lock.json  (if it exists) and\n    package.json .\n   This will be used later to determine if dependencies need to be installed.  It will then set some env vars for the Node dependency directories.\n1.  Clean and install project dependencies  ( clean_and_install )  The script will now compare the checksums to see if it needs to install\n   dependencies, or if they are already up-to-date.  If the checksums do not match, the script will empty out all installed\n   dependencies ( clean ) so the new installation can start fresh,\n   then install the latest requested dependencies ( install ).  The  devDependencies  from  package.json  are not installed\n   if the environment is production, and if it's the dev or test environment,\n   it checks to see if Protractor is globally installed.  Finally, it creates a new checksum for future comparisons.\n1.  Run tasks to build the project for distribution  ( build )  Finally, the script runs  gulp build  to rebuild the front-end assets.\n   It no longer cleans first, because the gulp-changed plugin prevents\n   rebuilding assets that haven't changed since the last build.  If this is the production environment, it also triggers style and script\n   builds for  ondemand  and  nemo , which aren't part of a standard\n    gulp build .", 
            "title": "1. frontend.sh"
        }, 
        {
            "location": "/installation/#2-backendsh", 
            "text": "Note  backend.sh  is not used for our Docker setup.    Confirm environment  ( init )   This script first checks for an argument passed from the command line\n   that can trigger different options for different environments.\n   Since you ran it with no arguments, it will set up the dev environment.  It will then run a script to ensure that you're in a virtualenv.\n   If not, the script will end, to prevent you from accidentally installing\n   your Python dependencies globally.\n1.  Install project dependencies  ( install )  Python dependencies are installed into the virtualenv via pip.\n   Dependencies vary slightly depending on whether we're in dev, test, or prod.\n1.  Setup MySQL server  ( db_setup )  Finally, the script will start the MySQL server, if it's not already running,\n   run  create-mysql-db.sh  to create the database using\n   the variables given in  .env , if it's not already there,\n   and will run  initial-data.sh  to create the first Wagtail user\n   and load some basic initial data.", 
            "title": "2. backend.sh"
        }, 
        {
            "location": "/installation/#troubleshooting", 
            "text": "Here are some common issues and how you can fix them:", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/installation/#errors-referencing-south-or-other-python-errors", 
            "text": "Since moving to Django 1.8, we use Django's built-in migration engine,\nand we no longer use South.\nIf you're getting South errors, you probably have it installed globally.\nTo solve this, from outside the virtual environment, run  pip uninstall south .  If you're getting other kinds of Python errors (for example, when running tox),\nyou may even want to go as far as uninstalling all globally-installed\nPython packages:  pip freeze | grep -v \"^-e\" | xargs pip uninstall -y .\nAfter doing that, you'll need to reinstall virtualenv: pip install virtualenv virtualenvwrapper .", 
            "title": "Errors referencing South, or other Python errors:"
        }, 
        {
            "location": "/usage/", 
            "text": "Usage: Stand Alone\n\n\nIf not using the Vagrant box, you will generally have four tabs\n(or windows) open in your terminal, which will be used for:\n\n\n\n\nGit operations\n.\n    Perform Git operations and general development in the repository,\n    such as \ngit checkout master\n.\n\n\nElasticsearch\n.\n    Run an Elasticsearch (ES) instance.\n    See instructions \nbelow\n.\n\n\nDjango server\n. Start and stop the web server.\n    Server is started with \n./runserver.sh\n,\n    but see more details \nbelow\n.\n\n\nGulp watch\n.\n    Run the Gulp watch (\ngulp watch\n) task to automatically re-run the gulp\n    asset compilation tasks when their source files are changed.\n\n\n\n\nWhat follows are the specific steps for each of these tabs.\n\n\n1. Git operations\n\n\nFrom this tab you can do Git operations,\nsuch as checking out our master branch:\n\n\ngit checkout master\n\n\n\nUpdating all dependencies\n\n\nEach time you fetch from the upstream repository (this repo), run \n./setup.sh\n.\nThis setup script will remove and reinstall the project dependencies\nand rebuild the site's JavaScript and CSS assets.\n\n\n\n\nNote\n\n\nYou may also run \n./backend.sh\n or \n./frontend.sh\n\nif you only want to re-build the backend or front-end, respectively.\n\n\n\n\n2. Run Elasticsearch\n\n\n\n\nNote\n\n\nThis Elasticsearch tab (or window) might not be necessary if you opted for the \nlaunchd\n option when \ninstalling Elasticsearch\n.\n\n\n\n\nTo launch Elasticsearch, first find out where your Elasticsearch config file is located.\nYou can do this with \nHomebrew\n using:\n\n\nbrew info elasticsearch\n\n\n\nThe last line of that output should be the command you need to launch Elasticsearch with the\nproper path to its configuration file. For example, it may look like:\n\n\nelasticsearch --config=/Users/[YOUR MAC OSX USERNAME]/homebrew/opt/elasticsearch/config/elasticsearch.yml\n\n\n\n3. Load Indexes \n Launch Site\n\n\nFirst, move into the \ncfgov-refresh\n project directory\nand ready your environment:\n\n\n# Use the cfgov-refresh virtualenv.\nworkon cfgov-refresh\n\n# cd into this directory (if you aren't already there)\ncd cfgov-refresh\n\n\n\nIndex the latest content from the API output from a WordPress and Django back-end.\n\nThis requires the constants in \nStand alone installation\n to be set.\n\n\npython cfgov/manage.py sheer_index -r\n\n\n\n\n\nNote\n\n\nTo view the indexed content you can use a tool called\n\nelasticsearch-head\n.\n\n\n\n\nFrom the project root, start the Django server:\n\n\n./runserver.sh\n\n\n\n\n\nNote\n\n\nIf prompted to migrate database changes,\nstop the server with \nctrl\n + \nc\n and run these commands:\n\n\n\n\npython cfgov/manage.py migrate\n./initial-data.sh\n./runserver.sh\n\n\n\nTo set up a superuser in order to access the Wagtail admin:\n\n\npython cfgov/manage.py createsuperuser\n\n\n\nTo view the site browse to: \nhttp://localhost:8000\n\n\n\n\nUsing a different port\n\n\nIf you want to run the server at a port other than 8000 use\n\n\npython cfgov/manage.py runserver \nport number\n\n\nSpecify an alternate port number, e.g. \n8001\n.\n\n\n\n\nTo view the Wagtail admin login,\nbrowse to: \nhttp://localhost:8000/admin/login/\n\n\n\n\nUsing HTTPS locally\n\n\nTo access a local server using HTTPS use\n\n\n./runserver.sh ssl\n\n\nYou'll need to ignore any browser certificate errors.\n\n\n\n\n4. Launch the Gulp watch task\n\n\nTo watch for changes in the source code and automatically update the running site,\nopen a terminal and run:\n\n\ngulp build\ngulp watch\n\n\n\n\n\nNote\n\n\nThe watch task will only re-run the tasks that have changed files.\nAlso, you must run \ngulp build\n at least once before watching.\n\n\n\n\n\n\nServer error\n\n\nIf you get this message on the page when running \ngulp watch\n:\n\"A server error occurred.  Please contact the administrator.\"\nYou likely need to delete files with the \n.pyc\n extension from the project with the following command:  \n\n\nfind . -name \\\"*.pyc\\\" -delete\n\n\n\n\nAvailable Gulp Tasks\n\n\nIn addition to \ngulp watch\n, there are a number of other important gulp tasks,\nparticularly \ngulp build\n and \ngulp test\n,\nwhich will build the project and test it, respectively.\nUsing the \ngulp --tasks\n command you can view all available tasks.\nThe important ones are listed below:\n\n\ngulp build           # Concatenate, optimize, and copy source files to the production /dist/ directory.\ngulp clean           # Remove the contents of the production /dist/ directory.\ngulp lint            # Lint the scripts and build files.\ngulp docs            # Generate JSDocs from the scripts.\ngulp test            # Run linting, unit and acceptance tests (see below).\ngulp test:unit       # Run only unit tests on source code.\ngulp test:acceptance # Run only acceptance (in-browser) tests on production code.\ngulp watch           # Watch for source code changes and auto-update a browser instance.\n\n\n\nUsage: Docker\n\n\nMuch of the guidance above for the \"stand-alone\" set-up still stands, and it \nis worth reviewing in full. Here are some things that might be different:\n\n\n\n\ndocker-compose\n takes care of running Elasticsearch for you, and all \nElastisearch, MySQL, and Python output will be shown in a single Terminal \nwindow or tab. (whereever you run \ndocker-compose up\n)\n\n\nmanage.py\ncommands can only be run after you've opened up a terminal in the \nPython container, which you can do with \n./shell.sh\n\n\nThere is not \nyet\n a good way to use SSL/HTTPS, but that is in the works\n\n\nYou won't ever use these scripts: \nsetup.sh\n, \nbackend.sh\n, \nrunserver.sh\n\n\n\n\nHow do I...\n\n\nUse Docker Machine\n\n\nIf you used \nmac-virtualbox-init.sh\n or \nsetup.sh docker\n, then we used Docker \nMachine to create a virtualbox VM, running the docker server. Here are some \nuseful docker machine commands:\n\n\n\n\nStart and stop the VM with  \ndocker-machine start\n and \ndocker-machine stop\n\n\nget the current machine IP with \ndocker-machine ip\n\n\nif for some reason you want to start over, \ndocker-machine rm default\n, and \n  \nsource mac-virtualbox-init.sh\n\n\n\n\nYou'll need to run this command in any new terminal window or tab:\n\n\neval $(docker-machine env)\n\n\nIt may be helpful to run \ndocker-machine env\n by itself, so you understand \nwhat's happening. Those variables are what allows docker-compose and the docker \ncommand line tool, running natively on your mac, to connect to the Docker \nserver running inside virtualbox.\n\n\nIf you use autoenv (described in the stand-alone intructions) or something \nsimilar, you might consider adding \neval $(docker-machine env)\n to your .env \nfile. You could also achieve the same results (and start the VM if it's not \nrunning yet) with \nsource mac-virtualbox-init.sh\n\n\nAny further Docker documentation will assume you are either in a shell where \nyou have already run \neval $(docker-machine env)\n, or you are in an environment \nwhere that's not neccessary.\n\n\nRun manage.py commands like migrate, shell, and dbshell, and shell scripts like refresh-data.sh\n\n\nrun \n./shell.sh\n to open up a shell \ninside\n the Python container. From there, \ncommands like \ncfgov/manage.py migrate\n should run as expected.\n\n\nThe same goes for scripts like \n./refresh-data.sh\n and \n./initial-data.sh\n -- \nthey will work as expected once you're inside the container.\n\n\nIn addition you can run single commands by passing them as arguments to \n\nshell.sh\n, for example:\n\n\n./shell.sh cfgov/manage.py migrate\n\n\nUse PDB\n\n\nRun \n./attach.sh\n to connect to the TTY session where \nmanage.py runserver\n is \nrunning. If the app is paused at a PDB prompt, this is where you can access it.\n\n\nHandle updates to Python requirements\n\n\nIf Compose is running, stop it with CTRL-C. Run:\n\n\ndocker-compose build python\n\n\nThis will update your Python image. The next time you run \ndocker-compose up\n, \nthe new requirements will be in place.\n\n\nSet environment variables\n\n\nYour shell environment variables (and the variables in your .env file, if you \nare using one) are not visible to applications running in Docker. If you need \nto set variables that will be visible to Django, and in \n./shell.sh\n, you'll \nneed to set them in the .python_env file, and restart the python container (it \nmight be simpler to simple stop compose with ctrl-c, and start it again with \n\ndocker-compose up\n)\n\n\n.python_env is \nnot\n a shell script, like your .env file, ~/.bash_profile, etc.\nSee the \nDocker Compose docs\n\n\nGet familiar with Docker Compose, and our configuration\n\n\ndocker-compose.yml contains a sort of \"recipe\" for running the site. Each entry \nin the Compose file describes a component of our application stack (MySQL, \nElasticsearch, and Python), and either points to a public image on Dockerhub, \nor to a Dockerfile in cfgov-refresh. You can learn a lot more about Compose \nfiles in \nthe docs\n\n\nSimilarly, a Dockerfile contains instructions for transforming some base image, \nto one that suits our needs. The Dockerfile sitting in the top level of \ncfgov-refresh is probably the most interesting. It starts with \n\nthe public CentOS:7 image\n, and installs \neverything else neccessary to run our Python dependencies and the Django app \nitself.  This file will only be executed:\n\n\n\n\nthe first time you run \ndocker-compose up\n (or the first time after you \nre-create the Docker Machine VM)\n\n\nany time you run \ndocker-compose build\n\n\n\n\nThat's why you need to run \ndocker-compose build\n after any changes to \n/requirements/\n\n\nThere are other compose subcommands you might be interested in. Consider \n\nlearning about\n \n\nbuild\n, \nrestarts\n, \nlogs\n, \nps\n, \ntop\n, and the \n-d\n option for \nup\n.\n\n\nDevelop satellite apps\n\n\nCheck out any apps you are developing into the develop-apps directory. These \nwill automatically be added to the \n\nPYTHONPATH\n, \nand apps contained within will be importable from Python running in the \ncontainer.\n\n\nFor example, if your app is called 'foobar', in a repo called foobar-project, \nyou could clone foobar-project in to develop apps:\n\n\ngit clone https://github.com/myorg/foobar-project\n\n\n... which will create a directory at develop-apps/foobar-project. Assuming \n'foobar' is at the top-level of 'foobar-project', you should be able to \nimport it from your python code:\n\n\nimport foobar\n\n\n\nrunserver has crashed! How do I start it again\n\n\nIn a seperate terminal window or tab, \ndocker-compose up python\n should restart \nit.", 
            "title": "Usage"
        }, 
        {
            "location": "/usage/#usage-stand-alone", 
            "text": "If not using the Vagrant box, you will generally have four tabs\n(or windows) open in your terminal, which will be used for:   Git operations .\n    Perform Git operations and general development in the repository,\n    such as  git checkout master .  Elasticsearch .\n    Run an Elasticsearch (ES) instance.\n    See instructions  below .  Django server . Start and stop the web server.\n    Server is started with  ./runserver.sh ,\n    but see more details  below .  Gulp watch .\n    Run the Gulp watch ( gulp watch ) task to automatically re-run the gulp\n    asset compilation tasks when their source files are changed.   What follows are the specific steps for each of these tabs.", 
            "title": "Usage: Stand Alone"
        }, 
        {
            "location": "/usage/#1-git-operations", 
            "text": "From this tab you can do Git operations,\nsuch as checking out our master branch:  git checkout master", 
            "title": "1. Git operations"
        }, 
        {
            "location": "/usage/#updating-all-dependencies", 
            "text": "Each time you fetch from the upstream repository (this repo), run  ./setup.sh .\nThis setup script will remove and reinstall the project dependencies\nand rebuild the site's JavaScript and CSS assets.   Note  You may also run  ./backend.sh  or  ./frontend.sh \nif you only want to re-build the backend or front-end, respectively.", 
            "title": "Updating all dependencies"
        }, 
        {
            "location": "/usage/#2-run-elasticsearch", 
            "text": "Note  This Elasticsearch tab (or window) might not be necessary if you opted for the  launchd  option when  installing Elasticsearch .   To launch Elasticsearch, first find out where your Elasticsearch config file is located.\nYou can do this with  Homebrew  using:  brew info elasticsearch  The last line of that output should be the command you need to launch Elasticsearch with the\nproper path to its configuration file. For example, it may look like:  elasticsearch --config=/Users/[YOUR MAC OSX USERNAME]/homebrew/opt/elasticsearch/config/elasticsearch.yml", 
            "title": "2. Run Elasticsearch"
        }, 
        {
            "location": "/usage/#3-load-indexes-launch-site", 
            "text": "First, move into the  cfgov-refresh  project directory\nand ready your environment:  # Use the cfgov-refresh virtualenv.\nworkon cfgov-refresh\n\n# cd into this directory (if you aren't already there)\ncd cfgov-refresh  Index the latest content from the API output from a WordPress and Django back-end. This requires the constants in  Stand alone installation  to be set.  python cfgov/manage.py sheer_index -r   Note  To view the indexed content you can use a tool called elasticsearch-head .   From the project root, start the Django server:  ./runserver.sh   Note  If prompted to migrate database changes,\nstop the server with  ctrl  +  c  and run these commands:   python cfgov/manage.py migrate\n./initial-data.sh\n./runserver.sh  To set up a superuser in order to access the Wagtail admin:  python cfgov/manage.py createsuperuser  To view the site browse to:  http://localhost:8000   Using a different port  If you want to run the server at a port other than 8000 use  python cfgov/manage.py runserver  port number  Specify an alternate port number, e.g.  8001 .   To view the Wagtail admin login,\nbrowse to:  http://localhost:8000/admin/login/   Using HTTPS locally  To access a local server using HTTPS use  ./runserver.sh ssl  You'll need to ignore any browser certificate errors.", 
            "title": "3. Load Indexes &amp; Launch Site"
        }, 
        {
            "location": "/usage/#4-launch-the-gulp-watch-task", 
            "text": "To watch for changes in the source code and automatically update the running site,\nopen a terminal and run:  gulp build\ngulp watch   Note  The watch task will only re-run the tasks that have changed files.\nAlso, you must run  gulp build  at least once before watching.    Server error  If you get this message on the page when running  gulp watch :\n\"A server error occurred.  Please contact the administrator.\"\nYou likely need to delete files with the  .pyc  extension from the project with the following command:    find . -name \\\"*.pyc\\\" -delete", 
            "title": "4. Launch the Gulp watch task"
        }, 
        {
            "location": "/usage/#available-gulp-tasks", 
            "text": "In addition to  gulp watch , there are a number of other important gulp tasks,\nparticularly  gulp build  and  gulp test ,\nwhich will build the project and test it, respectively.\nUsing the  gulp --tasks  command you can view all available tasks.\nThe important ones are listed below:  gulp build           # Concatenate, optimize, and copy source files to the production /dist/ directory.\ngulp clean           # Remove the contents of the production /dist/ directory.\ngulp lint            # Lint the scripts and build files.\ngulp docs            # Generate JSDocs from the scripts.\ngulp test            # Run linting, unit and acceptance tests (see below).\ngulp test:unit       # Run only unit tests on source code.\ngulp test:acceptance # Run only acceptance (in-browser) tests on production code.\ngulp watch           # Watch for source code changes and auto-update a browser instance.", 
            "title": "Available Gulp Tasks"
        }, 
        {
            "location": "/usage/#usage-docker", 
            "text": "Much of the guidance above for the \"stand-alone\" set-up still stands, and it \nis worth reviewing in full. Here are some things that might be different:   docker-compose  takes care of running Elasticsearch for you, and all \nElastisearch, MySQL, and Python output will be shown in a single Terminal \nwindow or tab. (whereever you run  docker-compose up )  manage.py commands can only be run after you've opened up a terminal in the \nPython container, which you can do with  ./shell.sh  There is not  yet  a good way to use SSL/HTTPS, but that is in the works  You won't ever use these scripts:  setup.sh ,  backend.sh ,  runserver.sh", 
            "title": "Usage: Docker"
        }, 
        {
            "location": "/usage/#how-do-i", 
            "text": "", 
            "title": "How do I..."
        }, 
        {
            "location": "/usage/#use-docker-machine", 
            "text": "If you used  mac-virtualbox-init.sh  or  setup.sh docker , then we used Docker \nMachine to create a virtualbox VM, running the docker server. Here are some \nuseful docker machine commands:   Start and stop the VM with   docker-machine start  and  docker-machine stop  get the current machine IP with  docker-machine ip  if for some reason you want to start over,  docker-machine rm default , and \n   source mac-virtualbox-init.sh   You'll need to run this command in any new terminal window or tab:  eval $(docker-machine env)  It may be helpful to run  docker-machine env  by itself, so you understand \nwhat's happening. Those variables are what allows docker-compose and the docker \ncommand line tool, running natively on your mac, to connect to the Docker \nserver running inside virtualbox.  If you use autoenv (described in the stand-alone intructions) or something \nsimilar, you might consider adding  eval $(docker-machine env)  to your .env \nfile. You could also achieve the same results (and start the VM if it's not \nrunning yet) with  source mac-virtualbox-init.sh  Any further Docker documentation will assume you are either in a shell where \nyou have already run  eval $(docker-machine env) , or you are in an environment \nwhere that's not neccessary.", 
            "title": "Use Docker Machine"
        }, 
        {
            "location": "/usage/#run-managepy-commands-like-migrate-shell-and-dbshell-and-shell-scripts-like-refresh-datash", 
            "text": "run  ./shell.sh  to open up a shell  inside  the Python container. From there, \ncommands like  cfgov/manage.py migrate  should run as expected.  The same goes for scripts like  ./refresh-data.sh  and  ./initial-data.sh  -- \nthey will work as expected once you're inside the container.  In addition you can run single commands by passing them as arguments to  shell.sh , for example:  ./shell.sh cfgov/manage.py migrate", 
            "title": "Run manage.py commands like migrate, shell, and dbshell, and shell scripts like refresh-data.sh"
        }, 
        {
            "location": "/usage/#use-pdb", 
            "text": "Run  ./attach.sh  to connect to the TTY session where  manage.py runserver  is \nrunning. If the app is paused at a PDB prompt, this is where you can access it.", 
            "title": "Use PDB"
        }, 
        {
            "location": "/usage/#handle-updates-to-python-requirements", 
            "text": "If Compose is running, stop it with CTRL-C. Run:  docker-compose build python  This will update your Python image. The next time you run  docker-compose up , \nthe new requirements will be in place.", 
            "title": "Handle updates to Python requirements"
        }, 
        {
            "location": "/usage/#set-environment-variables", 
            "text": "Your shell environment variables (and the variables in your .env file, if you \nare using one) are not visible to applications running in Docker. If you need \nto set variables that will be visible to Django, and in  ./shell.sh , you'll \nneed to set them in the .python_env file, and restart the python container (it \nmight be simpler to simple stop compose with ctrl-c, and start it again with  docker-compose up )  .python_env is  not  a shell script, like your .env file, ~/.bash_profile, etc.\nSee the  Docker Compose docs", 
            "title": "Set environment variables"
        }, 
        {
            "location": "/usage/#get-familiar-with-docker-compose-and-our-configuration", 
            "text": "docker-compose.yml contains a sort of \"recipe\" for running the site. Each entry \nin the Compose file describes a component of our application stack (MySQL, \nElasticsearch, and Python), and either points to a public image on Dockerhub, \nor to a Dockerfile in cfgov-refresh. You can learn a lot more about Compose \nfiles in  the docs  Similarly, a Dockerfile contains instructions for transforming some base image, \nto one that suits our needs. The Dockerfile sitting in the top level of \ncfgov-refresh is probably the most interesting. It starts with  the public CentOS:7 image , and installs \neverything else neccessary to run our Python dependencies and the Django app \nitself.  This file will only be executed:   the first time you run  docker-compose up  (or the first time after you \nre-create the Docker Machine VM)  any time you run  docker-compose build   That's why you need to run  docker-compose build  after any changes to \n/requirements/  There are other compose subcommands you might be interested in. Consider  learning about   build ,  restarts ,  logs ,  ps ,  top , and the  -d  option for  up .", 
            "title": "Get familiar with Docker Compose, and our configuration"
        }, 
        {
            "location": "/usage/#develop-satellite-apps", 
            "text": "Check out any apps you are developing into the develop-apps directory. These \nwill automatically be added to the  PYTHONPATH , \nand apps contained within will be importable from Python running in the \ncontainer.  For example, if your app is called 'foobar', in a repo called foobar-project, \nyou could clone foobar-project in to develop apps:  git clone https://github.com/myorg/foobar-project  ... which will create a directory at develop-apps/foobar-project. Assuming \n'foobar' is at the top-level of 'foobar-project', you should be able to \nimport it from your python code:  import foobar", 
            "title": "Develop satellite apps"
        }, 
        {
            "location": "/usage/#runserver-has-crashed-how-do-i-start-it-again", 
            "text": "In a seperate terminal window or tab,  docker-compose up python  should restart \nit.", 
            "title": "runserver has crashed! How do I start it again"
        }, 
        {
            "location": "/whats-new/", 
            "text": "What\u2019s new?\n\n\nGuiding principles\n\n\n\n\nReverse the sprawl of technologies and repositories left by the last 5 years of consumerfinance.gov development. \n\n\nReduce risk by integrating earlier and implementing a single release cadence for the entire site.\n\n\nWe don\u2019t deploy code to coincide with announcements, and events. \n\n\nPrefer using and/or extending the CMS and it\u2019s primitives (page types, atomic design elements, etc) over the creation of \u201capps\u201d that own a particular URL space.\n\n\n\n\nBenefits\n\n\n\n\nBy centering most work on the cfgov-refresh, it becomes easier to understand what\u2019s happening, easier to communicate about the state of the site, easier to develop on, and simpler to deploy.\n\n\nBy working with the CMS, we empower editors to determine when a particular page or section goes live, and can provide a way to edit the incidental text on a page. \n\n\n\n\nPractical impacts\n\n\n\n\nAny new work that will appear on consumerfinance.gov should be built with Django, and (subject to guidelines below) live in the primary code repo for the site (cfgov-refresh). \n\n\nIf a particular page, section, or feature can not go live before a particular date or time, then it must be hidden with feature flags or controlled via the CMS. Simply merging code (or updating a dependency) must not result in such things being revealed.\n\n\n\n\nNew build approach\n\n\nFor projects being developed outside of cfgov-refresh\n, the relationship with the project (and particularly the \u2018build\u2019 server) is changing. Under the old system, we maintained a separate \u2018requirements\u2019 file for build, content, and production. The \u2018build\u2019 requirements generally grabbed the master branch of the app, and we would pin a particular version for content and production.\n\n\nWhat we want to do now is quite a bit different. These apps will be treated like any other python dependency (ie, always pinned to a particular version). The \u2018build\u2019 server will reflect the master branch of cfgov-refresh, but for all other projects will only reflect the current pinned version reflected in requirements.txt, and changing that version requires a pull request. We then move complete releases of the site through the QA and deployment process described below under \u201crelease cadence\u201d\n\n\nIn short: if your code is being maintained outside of cfgov-refresh, you will need to provide your own \u2018build\u2019 environment and CI pipeline. We are working with the delivery team to make this less painful than it sounds.", 
            "title": "What's new"
        }, 
        {
            "location": "/whats-new/#whats-new", 
            "text": "", 
            "title": "What\u2019s new?"
        }, 
        {
            "location": "/whats-new/#guiding-principles", 
            "text": "Reverse the sprawl of technologies and repositories left by the last 5 years of consumerfinance.gov development.   Reduce risk by integrating earlier and implementing a single release cadence for the entire site.  We don\u2019t deploy code to coincide with announcements, and events.   Prefer using and/or extending the CMS and it\u2019s primitives (page types, atomic design elements, etc) over the creation of \u201capps\u201d that own a particular URL space.", 
            "title": "Guiding principles"
        }, 
        {
            "location": "/whats-new/#benefits", 
            "text": "By centering most work on the cfgov-refresh, it becomes easier to understand what\u2019s happening, easier to communicate about the state of the site, easier to develop on, and simpler to deploy.  By working with the CMS, we empower editors to determine when a particular page or section goes live, and can provide a way to edit the incidental text on a page.", 
            "title": "Benefits"
        }, 
        {
            "location": "/whats-new/#practical-impacts", 
            "text": "Any new work that will appear on consumerfinance.gov should be built with Django, and (subject to guidelines below) live in the primary code repo for the site (cfgov-refresh).   If a particular page, section, or feature can not go live before a particular date or time, then it must be hidden with feature flags or controlled via the CMS. Simply merging code (or updating a dependency) must not result in such things being revealed.", 
            "title": "Practical impacts"
        }, 
        {
            "location": "/whats-new/#new-build-approach", 
            "text": "For projects being developed outside of cfgov-refresh , the relationship with the project (and particularly the \u2018build\u2019 server) is changing. Under the old system, we maintained a separate \u2018requirements\u2019 file for build, content, and production. The \u2018build\u2019 requirements generally grabbed the master branch of the app, and we would pin a particular version for content and production.  What we want to do now is quite a bit different. These apps will be treated like any other python dependency (ie, always pinned to a particular version). The \u2018build\u2019 server will reflect the master branch of cfgov-refresh, but for all other projects will only reflect the current pinned version reflected in requirements.txt, and changing that version requires a pull request. We then move complete releases of the site through the QA and deployment process described below under \u201crelease cadence\u201d  In short: if your code is being maintained outside of cfgov-refresh, you will need to provide your own \u2018build\u2019 environment and CI pipeline. We are working with the delivery team to make this less painful than it sounds.", 
            "title": "New build approach"
        }, 
        {
            "location": "/new-projects/", 
            "text": "Setting up new projects\n\n\nAll new code should start in the \ncfgov-refresh repository\n unless the following is true:\n\n\n\n\nIt does not require integration with CMS\n\n\nIt is not expected to match the look and feel of the larger site (in fact \"very different\" is preferable to \"almost the same\")\n\n\nIt must be pip installable like any other dependency. \n\n\n\n\nIf a project meets this criteria, it is important to note that while the app itself is not necessarily tied to the cf.gov platform\u2019s release cadence, its dependencies are. Such projects must also maintain it's own continuous integration pipeline and build server.\n\n\nFor everything else, all code in the master branch is subject to a regular release cadence. Features that must go live on a certain date should be hidden by feature flags. Deployments should not be timed to coincide with announcements, press releases, speeches, or other events. The code should be already deployed and waiting for the feature to be turned on by a site manager. See \u201cFeature Flags\u201d.\n\n\nRather than using \u201cclassic\u201d Django views added to urls.py, when feasible an app should provide singleton Wagtail Page. See \u201cWagtail Pages\u201d.\n\n\nDecision Matrix\n\n\n\n\n\n\n\n\nProduct\n\n\nContent Pages\n\n\nAPIs\n\n\nHTML5 API Clients (\"single page apps\")\n\n\nTraditional Web Apps\n\n\n\n\n\n\n\n\n\n\nWhat to build\n\n\nWagtail page types and templates\n\n\nDjango app using Django REST Framework\n\n\nThe API (if needed) and Wagtail page type and template to host the client\n\n\nA standard models / forms / views-based Django app. Often calling internal/external APIs\n\n\n\n\n\n\nWhere does the code live\n\n\ncfgov-refresh\n\n\ncfgov-refresh (see exceptions)\n\n\ncfgov-refresh\n\n\ncfgov-refresh (see exceptions)\n\n\n\n\n\n\nHow to start\n\n\nExtend our existing library of page types and molecules\n\n\ncreate a Django app in the \"api's\" namespace\n\n\nBuild the API first, then create a wagtail page to host the tool\n\n\nCreate a Django app at the top level of the repo. When feasible, consider providing Wagtail page-types instead of traditional views\n\n\n\n\n\n\nHow to ship\n\n\nWith release cycle\n\n\nWith release cycle\n\n\nConsider getting APIs deployed well in advance\n\n\nwith release cycle\n\n\n\n\n\n\nHow to change\n\n\nSee below\n\n\nWith the release cycle\n\n\nUse \nAPI versioning\n to avoid breaking existing code\n\n\nSee below\n\n\n\n\n\n\n\n\nHow to handle Wagtail changes\n\n\nMinor visual updates to a page\n\n\n\n\nmake the changes as part of the release cadence\n\n\n\n\nExtensive visual changes to a page\n\n\n\n\ncreate a new template to reflect the new design\n\n\nedit the page type to allow for switching between old and new templates.\n\n\non build and staging servers, feel free to switch back and forth between the designs\n\n\n\n\nVisual and minor data model changes\n\n\n\n\nmake the data model changes in such a way that doesn't break the current template\n\n\nfollow the \"extensive visual changes to a page\" guidance above to make the visual changes\n\n\n\n\nMajor data model changes\n\n\n\n\ncreate a new page type, and the corresponding template\n\n\nEditors can then replace the old page with the new one, at will\n\n\n\n\nFront-end Resources\n\n\nFront-end resources should conform to \nCFPB development standards\n and should use atomic elements, organisms, existing structure and convention. When applicable, front-end components should be added to Capital Framework using atomic design principles.\n\n\nProjects that will be part of cfgov but live in their own repositories should:\n\n\n\n\nfollow file-naming conventions for their front-end resources to avoid collisions (eg, project_name.js rather than main.js) OR follow resource folder structure conventions\n\n\nwhere possible, follow front-end guidelines/templates for new projects, including build processes and testing setup\n\n\nwhere necessary, follow a suggested approach for sharing resources (JavaScript/CSS) that are internal to cfgov-refresh", 
            "title": "New projects"
        }, 
        {
            "location": "/new-projects/#setting-up-new-projects", 
            "text": "All new code should start in the  cfgov-refresh repository  unless the following is true:   It does not require integration with CMS  It is not expected to match the look and feel of the larger site (in fact \"very different\" is preferable to \"almost the same\")  It must be pip installable like any other dependency.    If a project meets this criteria, it is important to note that while the app itself is not necessarily tied to the cf.gov platform\u2019s release cadence, its dependencies are. Such projects must also maintain it's own continuous integration pipeline and build server.  For everything else, all code in the master branch is subject to a regular release cadence. Features that must go live on a certain date should be hidden by feature flags. Deployments should not be timed to coincide with announcements, press releases, speeches, or other events. The code should be already deployed and waiting for the feature to be turned on by a site manager. See \u201cFeature Flags\u201d.  Rather than using \u201cclassic\u201d Django views added to urls.py, when feasible an app should provide singleton Wagtail Page. See \u201cWagtail Pages\u201d.", 
            "title": "Setting up new projects"
        }, 
        {
            "location": "/new-projects/#decision-matrix", 
            "text": "Product  Content Pages  APIs  HTML5 API Clients (\"single page apps\")  Traditional Web Apps      What to build  Wagtail page types and templates  Django app using Django REST Framework  The API (if needed) and Wagtail page type and template to host the client  A standard models / forms / views-based Django app. Often calling internal/external APIs    Where does the code live  cfgov-refresh  cfgov-refresh (see exceptions)  cfgov-refresh  cfgov-refresh (see exceptions)    How to start  Extend our existing library of page types and molecules  create a Django app in the \"api's\" namespace  Build the API first, then create a wagtail page to host the tool  Create a Django app at the top level of the repo. When feasible, consider providing Wagtail page-types instead of traditional views    How to ship  With release cycle  With release cycle  Consider getting APIs deployed well in advance  with release cycle    How to change  See below  With the release cycle  Use  API versioning  to avoid breaking existing code  See below", 
            "title": "Decision Matrix"
        }, 
        {
            "location": "/new-projects/#how-to-handle-wagtail-changes", 
            "text": "", 
            "title": "How to handle Wagtail changes"
        }, 
        {
            "location": "/new-projects/#minor-visual-updates-to-a-page", 
            "text": "make the changes as part of the release cadence", 
            "title": "Minor visual updates to a page"
        }, 
        {
            "location": "/new-projects/#extensive-visual-changes-to-a-page", 
            "text": "create a new template to reflect the new design  edit the page type to allow for switching between old and new templates.  on build and staging servers, feel free to switch back and forth between the designs", 
            "title": "Extensive visual changes to a page"
        }, 
        {
            "location": "/new-projects/#visual-and-minor-data-model-changes", 
            "text": "make the data model changes in such a way that doesn't break the current template  follow the \"extensive visual changes to a page\" guidance above to make the visual changes", 
            "title": "Visual and minor data model changes"
        }, 
        {
            "location": "/new-projects/#major-data-model-changes", 
            "text": "create a new page type, and the corresponding template  Editors can then replace the old page with the new one, at will", 
            "title": "Major data model changes"
        }, 
        {
            "location": "/new-projects/#front-end-resources", 
            "text": "Front-end resources should conform to  CFPB development standards  and should use atomic elements, organisms, existing structure and convention. When applicable, front-end components should be added to Capital Framework using atomic design principles.  Projects that will be part of cfgov but live in their own repositories should:   follow file-naming conventions for their front-end resources to avoid collisions (eg, project_name.js rather than main.js) OR follow resource folder structure conventions  where possible, follow front-end guidelines/templates for new projects, including build processes and testing setup  where necessary, follow a suggested approach for sharing resources (JavaScript/CSS) that are internal to cfgov-refresh", 
            "title": "Front-end Resources"
        }, 
        {
            "location": "/branching-merging/", 
            "text": "Branching and merging\n\n\nBranches should be named descriptively, preferably in some way that indicates whether they are short-lived feature branches or longer-lived development branches. Short-lived feature branches should be deleted once they are merged into master. \n\n\nAll pull requests to merge into master must be reviewed by at least one member of the cf.gov platform team. The cf.gov platform team will ensure that these reviews happen in a timely manner. To ensure timely code reviews, please tag all PRs to master with @cfpb/cfgov-backends and @cfpb/cfgov-frontends as appropriate.\n\n\nWhen reviewing pull requests, it is important to distinguish between explicit blockers and things that can be addressed in the future or would be nice to have. The latter two can be indicated with 'TODO'. This is best as a simple top-level post after review to summarize the review.\n\n\nThe cfgov-refresh repository makes use of automated testing and linting to ensure the quality, consistency, and readability of the codebase. All pull requests to master must pass all automated tests and must not reduce the code coverage of the codebase. It is the responsibility of the submitter to ensure that the tests pass.\n\n\nPull requests that are \nnot\n to master must use GitHub labels in such a way that individuals who are responsible for reviewing those pull requests can easily find them. Pull requests that are works-in-progress must be clearly labeled as such.\n\n\nGenerally, teams working on cf.gov projects should create and collaborate on feature branches, with frequent merges back to master. Teams are responsible for governing their own branches and forks, these guidelines apply to master.", 
            "title": "Branching and merging"
        }, 
        {
            "location": "/branching-merging/#branching-and-merging", 
            "text": "Branches should be named descriptively, preferably in some way that indicates whether they are short-lived feature branches or longer-lived development branches. Short-lived feature branches should be deleted once they are merged into master.   All pull requests to merge into master must be reviewed by at least one member of the cf.gov platform team. The cf.gov platform team will ensure that these reviews happen in a timely manner. To ensure timely code reviews, please tag all PRs to master with @cfpb/cfgov-backends and @cfpb/cfgov-frontends as appropriate.  When reviewing pull requests, it is important to distinguish between explicit blockers and things that can be addressed in the future or would be nice to have. The latter two can be indicated with 'TODO'. This is best as a simple top-level post after review to summarize the review.  The cfgov-refresh repository makes use of automated testing and linting to ensure the quality, consistency, and readability of the codebase. All pull requests to master must pass all automated tests and must not reduce the code coverage of the codebase. It is the responsibility of the submitter to ensure that the tests pass.  Pull requests that are  not  to master must use GitHub labels in such a way that individuals who are responsible for reviewing those pull requests can easily find them. Pull requests that are works-in-progress must be clearly labeled as such.  Generally, teams working on cf.gov projects should create and collaborate on feature branches, with frequent merges back to master. Teams are responsible for governing their own branches and forks, these guidelines apply to master.", 
            "title": "Branching and merging"
        }, 
        {
            "location": "/release-cadence/", 
            "text": "Release Cadence\n\n\ncf.gov is on a two-week release cadence. The release process is as follows:\n\n\n\n\nA new branch is created from the master branch representing the next minor release, e.g. 5.1. \n\n\nA new release is tagged from that branch with a 0 patch number, e.g. 5.1.0.\n\n\nThat release is deployed to our beta server (\"beta\").\n\n\nIf any fixes are necessary before going to production, they are committed to the release branch, and back-merged to master. New \"hotfix\" releases are tagged from the branch with the appropriate patch number as needed to get urgent changes onto beta.\n\n\nThe latest release tagged on the release branch is deployed to production servers (\"content\" and \"www\"). \n\n\nIf any urgent changes are needed before the next release is deployed to www and content we follow the same \"hotfix\" procedure detailed above for beta.\n\n\n\n\n\n\nSample schedule\n\n\n\n\n\n\n\n\nMonday\n\n\nTuesday\n\n\nWednesday\n\n\nThursday\n\n\nFriday\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1 branched, 5.1.0 released, deployed to beta\n\n\nHotfix 5.1.1 committed, released, deployed to beta\n\n\n\n\n\n\n\n\n5.1.1 deployed to www and content\n\n\nHotfix 5.1.2 committed, released, deployed to www and content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2 branched, 5.2.0 released, deployed to beta", 
            "title": "Release cadence"
        }, 
        {
            "location": "/release-cadence/#release-cadence", 
            "text": "cf.gov is on a two-week release cadence. The release process is as follows:   A new branch is created from the master branch representing the next minor release, e.g. 5.1.   A new release is tagged from that branch with a 0 patch number, e.g. 5.1.0.  That release is deployed to our beta server (\"beta\").  If any fixes are necessary before going to production, they are committed to the release branch, and back-merged to master. New \"hotfix\" releases are tagged from the branch with the appropriate patch number as needed to get urgent changes onto beta.  The latest release tagged on the release branch is deployed to production servers (\"content\" and \"www\").   If any urgent changes are needed before the next release is deployed to www and content we follow the same \"hotfix\" procedure detailed above for beta.", 
            "title": "Release Cadence"
        }, 
        {
            "location": "/release-cadence/#sample-schedule", 
            "text": "Monday  Tuesday  Wednesday  Thursday  Friday         5.1 branched, 5.1.0 released, deployed to beta  Hotfix 5.1.1 committed, released, deployed to beta     5.1.1 deployed to www and content  Hotfix 5.1.2 committed, released, deployed to www and content         5.2 branched, 5.2.0 released, deployed to beta", 
            "title": "Sample schedule"
        }, 
        {
            "location": "/deployment/", 
            "text": "Deployment\n\n\nEverything that is part of cfgov-refresh and its dependencies are deployed as part of the cf.gov deployment jobs. For needs that are outside of the standard deployment process (perhaps relating to regular data loading or manipulation), additional jobs will need to be created.\n\n\nProjects that are independent of cfgov-refresh will need to provide their own deployment process, and all jobs they require. They will not be automatically included in the cf.gov deployment process, and will not be on the cf.gov release cadence. \n\n\nAll new deployment jobs, jobs that run in addition to the cf.gov deployment jobs, as well as independent jobs, must be implemented with \nJenkins-as-code\n.\n\n\nDeployment QA\n\n\nAll code that gets merged into the cfgov-refresh master must have adequate tests, as appropriate for the nature of the code. \n\n\nThis could potentially include unit tests, browser tests, and 508-compliance tests.\n\n\nThere should be no drop in test coverage for cfgov-refresh.\n\n\nRegular releases of cf.gov on our release cadence are automated, presuming:\n\n\n\n\nAll unit tests pass \n\n\nAll functional tests pass\n\n\nThere is no reduction in test coverage from the last release tagging\n\n\n\n\nCurrent deployment process\n\n\nThe current cf.gov deployment process requires running the cf.gov-pipeline-build Jenkins pipeline job, with the release tag to deploy and the environment to which it should be deployed.\n\n\nThe pipeline will invoke the following jobs, in order:\n\n\n\n\ncf.gov-frontend-build, which builds the front-end assets for cfgov-refresh\n\n\ncf.gov-deploy-django, which takes the cfgov-refresh build, and uses \ndrama-free-django\n to build a deployable artifact for all of cf.gov and its requirements, and then deploys the artifact to the appropriate servers for the selected environment.\n\n\n\n\nAny back-end developer on the platform team should be able to assist with deployments. You may also contact the following individually:\n\n\n\n\nRoss Karchner\n\n\nScott Cranfill\n\n\nWill Barton\n\n\nBill Higgins\n\n\nSerghei Gorobet\n\n\nThe Software Delivery team", 
            "title": "Deployment"
        }, 
        {
            "location": "/deployment/#deployment", 
            "text": "Everything that is part of cfgov-refresh and its dependencies are deployed as part of the cf.gov deployment jobs. For needs that are outside of the standard deployment process (perhaps relating to regular data loading or manipulation), additional jobs will need to be created.  Projects that are independent of cfgov-refresh will need to provide their own deployment process, and all jobs they require. They will not be automatically included in the cf.gov deployment process, and will not be on the cf.gov release cadence.   All new deployment jobs, jobs that run in addition to the cf.gov deployment jobs, as well as independent jobs, must be implemented with  Jenkins-as-code .", 
            "title": "Deployment"
        }, 
        {
            "location": "/deployment/#deployment-qa", 
            "text": "All code that gets merged into the cfgov-refresh master must have adequate tests, as appropriate for the nature of the code.   This could potentially include unit tests, browser tests, and 508-compliance tests.  There should be no drop in test coverage for cfgov-refresh.  Regular releases of cf.gov on our release cadence are automated, presuming:   All unit tests pass   All functional tests pass  There is no reduction in test coverage from the last release tagging", 
            "title": "Deployment QA"
        }, 
        {
            "location": "/deployment/#current-deployment-process", 
            "text": "The current cf.gov deployment process requires running the cf.gov-pipeline-build Jenkins pipeline job, with the release tag to deploy and the environment to which it should be deployed.  The pipeline will invoke the following jobs, in order:   cf.gov-frontend-build, which builds the front-end assets for cfgov-refresh  cf.gov-deploy-django, which takes the cfgov-refresh build, and uses  drama-free-django  to build a deployable artifact for all of cf.gov and its requirements, and then deploys the artifact to the appropriate servers for the selected environment.   Any back-end developer on the platform team should be able to assist with deployments. You may also contact the following individually:   Ross Karchner  Scott Cranfill  Will Barton  Bill Higgins  Serghei Gorobet  The Software Delivery team", 
            "title": "Current deployment process"
        }, 
        {
            "location": "/feature-flags/", 
            "text": "Feature flags\n\n\nFeature flags are implemented using our \nWagtail-Flags\n app. The \nREADME\n contains an overview and examples of how to use feature flags in Wagtail.\n\n\nThis document covers how to add and use feature flags with cfgov-refresh and the conventions we have around their use.\n\n\n\n\nAdding a flag\n\n\nChecking a flag\n\n\nIn templates\n\n\nJinja2\n\n\nDjango\n\n\n\n\n\n\nIn code\n\n\nIn URLs\n\n\n\n\n\n\nEnabling a flag\n\n\nHard-coded conditions\n\n\nDatabase conditions\n\n\n\n\n\n\nSatellite apps\n\n\nHygiene\n\n\n\n\nAdding a flag\n\n\nFeature flags are defined in code in the \ncfgov/settings/base.py\n file as part of the \nFLAGS\n setting. Each flag consists of a single string and a Python dictionary (\n{}\n) of its hard-coded conditions (see \nEnabling a flag\n below).\n\n\nFLAGS = {\n    # Beta banner, seen on beta.consumerfinance.gov\n    # When enabled, a banner appears across the top of the site proclaiming\n    # \nThis beta site is a work in progress.\n\n    'BETA_NOTICE': {},\n}\n\n\n\nBy convention our flag names are all uppercase, with underscores instead of whitespace. A comment is expected above each flag with a short description fo what happens when it is enabled.\n\n\nChecking a flag\n\n\nFlags can be checked either in Python code or in Django or Jinja2 template files. See the full \nWagtail Flags API is documented \n for more information.\n\n\nIn templates\n\n\nJinja2\n\n\nMost of cfgov-refresh's templates are Jinja2. In these templates, two template functions are provided, \nflag_enabled\n and \nflag_disabled\n. Each takes a flag name as its first argument and request` object as the second.\n\n\nflag_enabled('MY_FLAG', request)\n will return \nTrue\n if the conditions under which \nMY_FLAG\n is enabled \nare\n met.\n\n\nflag_disabled('MY_FLAG', request)\n will return \nTrue\n if the conditions under which \nMY_FLAG\n is enabled \nare not\n met.\n\n\nSee \nEnabling a flag\n below for more on flag conditions.\n\n\nAn example is \nthe \nBETA_NOTICE flag\n as implemented in \nheader.html\n:\n\n\n{% if flag_enabled('BETA_NOTICE', request) and show_banner %}\n\ndiv class=\nm-global-banner\n\n    \ndiv class=\nwrapper\n                wrapper__match-content\n                o-expandable\n                o-expandable__expanded\n\n        \ndiv class=\nm-global-banner_head\n\n            \nspan class=\ncf-icon\n                         cf-icon-error-round\n                         m-global-banner_icon\n/span\n\n            This beta site is a work in progress.\n        \n/div\n\n        \u2026\n    \n/div\n\n\n/div\n\n{% endif %}\n\n\n\nDjango\n\n\nIn Django templates (used in Satellite apps and the Wagtail admin), two template functions are provided \nflag_enabled\n and \nflag_disabled\n once the \nfeature_flags\n template tag library is loaded.\n\n\nflag_enabled 'MY_FLAG'\n will return \nTrue\n if the conditions under which \nMY_FLAG\n is enabled \nare\n met.\n\n\nflag_disabled 'MY_FLAG'\n will return \nTrue\n if the conditions under which \nMY_FLAG\n is enabled \nare not\n met.\n\n\nSee \nEnabling a flag\n below for more on flag conditions.\n\n\nThe \nBETA_NOTICE\n \nJinja2\n example above when implemented with Django templates would look like this:\n\n\n{% load feature_flags %}\n\n{% flag_enabled 'BETA_NOTICE' as beta_flag %}\n{% if beta_flag and show_banner %}\n\ndiv class=\nm-global-banner\n\n    \ndiv class=\nwrapper\n                wrapper__match-content\n                o-expandable\n                o-expandable__expanded\n\n        \ndiv class=\nm-global-banner_head\n\n            \nspan class=\ncf-icon\n                         cf-icon-error-round\n                         m-global-banner_icon\n/span\n\n            This beta site is a work in progress.\n        \n/div\n\n        \u2026\n    \n/div\n\n\n/div\n\n{% endif %}\n\n\n\nIn code\n\n\nIn Python code three functions are available for checking feature flags, \nflag_state\n, \nflag_enabled\n, and \nflag_disabled\n. The Python API is slightly different from the \nJinja2\n or \nDjango template\n API, in that flag conditions can take more potential arguments than requests, and thus flags are more flexible when checked in Python (in and outside a request cycle).\n\n\nSee the \nWagtail Flags flag state API documentation for more\n.\n\n\nAdditionally two decorators, \nflag_check\n and \nflag_required\n, are provided for wrapping views (and another functions) in a feature flag check.  See the \nWagtail Flags flag decorators API documentation for more\n.\n\n\nIn URLs\n\n\nThere are two ways to flag Django URL patterns in \nurls.py\n: with \nflagged_url()\n in place of \nurl()\n for a single pattern, or with the \nflagged_urls()\n context manager for multiple URLs.\n\n\nflagged_url(flag_name, regex, view, kwargs=None, name=None, state=True, fallback=None)\n works exactly like \nurl()\n except it takes a flag name as its first argument. If the flag's state matches the given \nstate\n, the URL pattern will be served from the given \nview\n; if not, and \nfallback\n is given, the \nfallback\n will be used.\n\n\nAn example is \nour \nWAGTAIL_ABOUT_US\n flag\n:\n\n\nflagged_url('WAGTAIL_ABOUT_US',\n            r'^about-us/$',\n            lambda req: ServeView.as_view()(req, req.path),\n            fallback=SheerTemplateView.as_view(\n                template_name='about-us/index.html'),\n            name='about-us'),\n\n\n\nIgnoring the \nview\n being a \nlambda\n for now (see \nFlagging Wagtail URLs below\n), this URL will be served via Wagtail if \nWAGTAIL_ABOUT_US\n's conditions are \nTrue\n, and from a \nTemplateView\n if its conditions are \nFalse\n.\n\n\nIf you need to flag multiple URLs with the same flag, you can use the \nflagged_urls()\n context manager.\n\n\nwith flagged_urls(flag_name, state=True, fallback=None) as url\n provides a context in which the returned \nurl()\n function can be used in place of the Django \nurl()\n function in patterns and those patterns will share the same feature flag, state, and fallback.\n\n\nAn example is \nour \nWAGTAIL_ASK_CFPB\n flag\n:\n\n\nwith flagged_urls('WAGTAIL_ASK_CFPB') as url:\n    ask_patterns = [\n        url(r'^(?i)ask-cfpb/([-\\w]{1,244})-(en)-(\\d{1,6})/?$',\n            view_answer,\n            name='ask-english-answer'),\n        url(r'^(?i)obtener-respuestas/([-\\w]{1,244})-(es)-(\\d{1,6})/?$',\n            view_answer,\n            name='ask-spanish-answer'),\n        \u2026\n    ]\n\nurlpatterns += ask_patterns\n\n\n\n\n\nWarning\n\n\nDo not attempt to use \nflag_check\n or any flag state-checking functions in \nurls.py\n. Because they will be evaluated on import of \nurls.py\n they will attempt to access the Django FlagState model before it is ready and will error.\n\n\n\n\nFlagging Wagtail URLs\n\n\nWagtail views in \nflagged_url\n with a Django view as fallback (or vice-versa) can be a bit awkward. Django views are typically called with \nrequest\n as the first argument, and Wagtail's \nserve\n view takes both the request and the path. To get around this, in \nflagged_url\n we typically use a \nlambda\n for the view:\n\n\nlambda req: ServeView.as_view()(req, req.path)\n\n\n\nThis lambda takes the request and calls the \nWagtail-Sharing\n \nServeView\n (which we're using in place of \nwagtail.wagtailcore.views.serve\n).\n\n\nEnabling a flag\n\n\nFeature flags are enabled based on a set of conditions that are given either in the Django settings files (in \ncfgov/cfgov/settings/\n) or in the Django or Wagtail admin. Multiple conditions can be given, both in settings and in the admin, and if any condition is satisfied a flag is enabled.\n\n\nA list of available conditions and how to use them is available in the Wagtail-Flags documentation\n.\n\n\nHard-coded conditions\n\n\nConditions that are defined in the Django settings are hard-coded, and require a change to files in cfgov-refresh, a new tagged release, and new deployment to change. These conditions should be used for flags that are relatively long-lasting and that can require a round-trip through the release and deployment process to change.\n\n\nWhen \nadding a flag\n to the Django settings the flag's dictionary of conditions can contain a condition name and value that must be satisfied for the flag to be enabled. The nature of that value changes depending on the condition type. \nSee the Wagtail-Flags conditions documentation\n for more on individual conditions.\n\n\nThere is a simple \nboolean\n condition that is either \nTrue\n or \nFalse\n, and if it is \nTrue\n the flag is enabled and if it is \nFalse\n the flag is disabled. If we want to always turn the \nBETA_NOTICE\n flag on in settings with a \nboolean\n condition, that would look like this:\n\n\nFLAGS = {\n    # Beta banner, seen on beta.consumerfinance.gov\n    # When enabled, a banner appears across the top of the site proclaiming\n    # \nThis beta site is a work in progress.\n\n    'BETA_NOTICE': {\n        'boolean': True,\n    },\n}\n\n\n\nDatabase conditions\n\n\nConditions that are managed via the Wagtail or Django admin are stored in the database. These conditions can be changed in real-time and do not require any code changes or release and deployment to change (presuming the code that uses the feature flag is in place).\n\n\nTo view, delete, and add database conditions, navigate to \"Settings \n Flags\" in the Wagtail admin.\n\n\n\n\nOnce in the flag settings, you'll have a list of all flags and their conditions..\n\n\n\n\nDatabase conditions can be deleted with the trash can button on the right.\n\n\nTo create a new database condition, select \"Add a condition\". As with \nhard-coded conditions\n, to create a database condition you must select which condition type you would like to use and give it a value that must be satisfied for the flag to be enabled.\n\n\n\n\nDatabase conditions can only be set for flags that exist in the Django settings.\n\n\nSatellite apps\n\n\nFeature flags can be used in satellite apps in exactly the same way they are used in cfgov-refresh. An example is \nthe use of a feature flagged template choice in the complaintdatabase app\n.\n\n\nHygiene\n\n\nFeature flags should be rare and ephemeral. Changes should be small and frequent, and not big-bang releases, and flags that are no longer used and their conditions should be cleaned up and removed from code and the database.", 
            "title": "Feature flags"
        }, 
        {
            "location": "/feature-flags/#feature-flags", 
            "text": "Feature flags are implemented using our  Wagtail-Flags  app. The  README  contains an overview and examples of how to use feature flags in Wagtail.  This document covers how to add and use feature flags with cfgov-refresh and the conventions we have around their use.   Adding a flag  Checking a flag  In templates  Jinja2  Django    In code  In URLs    Enabling a flag  Hard-coded conditions  Database conditions    Satellite apps  Hygiene", 
            "title": "Feature flags"
        }, 
        {
            "location": "/feature-flags/#adding-a-flag", 
            "text": "Feature flags are defined in code in the  cfgov/settings/base.py  file as part of the  FLAGS  setting. Each flag consists of a single string and a Python dictionary ( {} ) of its hard-coded conditions (see  Enabling a flag  below).  FLAGS = {\n    # Beta banner, seen on beta.consumerfinance.gov\n    # When enabled, a banner appears across the top of the site proclaiming\n    #  This beta site is a work in progress. \n    'BETA_NOTICE': {},\n}  By convention our flag names are all uppercase, with underscores instead of whitespace. A comment is expected above each flag with a short description fo what happens when it is enabled.", 
            "title": "Adding a flag"
        }, 
        {
            "location": "/feature-flags/#checking-a-flag", 
            "text": "Flags can be checked either in Python code or in Django or Jinja2 template files. See the full  Wagtail Flags API is documented   for more information.", 
            "title": "Checking a flag"
        }, 
        {
            "location": "/feature-flags/#in-templates", 
            "text": "", 
            "title": "In templates"
        }, 
        {
            "location": "/feature-flags/#jinja2", 
            "text": "Most of cfgov-refresh's templates are Jinja2. In these templates, two template functions are provided,  flag_enabled  and  flag_disabled . Each takes a flag name as its first argument and request` object as the second.  flag_enabled('MY_FLAG', request)  will return  True  if the conditions under which  MY_FLAG  is enabled  are  met.  flag_disabled('MY_FLAG', request)  will return  True  if the conditions under which  MY_FLAG  is enabled  are not  met.  See  Enabling a flag  below for more on flag conditions.  An example is  the  BETA_NOTICE flag  as implemented in  header.html :  {% if flag_enabled('BETA_NOTICE', request) and show_banner %} div class= m-global-banner \n     div class= wrapper\n                wrapper__match-content\n                o-expandable\n                o-expandable__expanded \n         div class= m-global-banner_head \n             span class= cf-icon\n                         cf-icon-error-round\n                         m-global-banner_icon /span \n            This beta site is a work in progress.\n         /div \n        \u2026\n     /div  /div \n{% endif %}", 
            "title": "Jinja2"
        }, 
        {
            "location": "/feature-flags/#django", 
            "text": "In Django templates (used in Satellite apps and the Wagtail admin), two template functions are provided  flag_enabled  and  flag_disabled  once the  feature_flags  template tag library is loaded.  flag_enabled 'MY_FLAG'  will return  True  if the conditions under which  MY_FLAG  is enabled  are  met.  flag_disabled 'MY_FLAG'  will return  True  if the conditions under which  MY_FLAG  is enabled  are not  met.  See  Enabling a flag  below for more on flag conditions.  The  BETA_NOTICE   Jinja2  example above when implemented with Django templates would look like this:  {% load feature_flags %}\n\n{% flag_enabled 'BETA_NOTICE' as beta_flag %}\n{% if beta_flag and show_banner %} div class= m-global-banner \n     div class= wrapper\n                wrapper__match-content\n                o-expandable\n                o-expandable__expanded \n         div class= m-global-banner_head \n             span class= cf-icon\n                         cf-icon-error-round\n                         m-global-banner_icon /span \n            This beta site is a work in progress.\n         /div \n        \u2026\n     /div  /div \n{% endif %}", 
            "title": "Django"
        }, 
        {
            "location": "/feature-flags/#in-code", 
            "text": "In Python code three functions are available for checking feature flags,  flag_state ,  flag_enabled , and  flag_disabled . The Python API is slightly different from the  Jinja2  or  Django template  API, in that flag conditions can take more potential arguments than requests, and thus flags are more flexible when checked in Python (in and outside a request cycle).  See the  Wagtail Flags flag state API documentation for more .  Additionally two decorators,  flag_check  and  flag_required , are provided for wrapping views (and another functions) in a feature flag check.  See the  Wagtail Flags flag decorators API documentation for more .", 
            "title": "In code"
        }, 
        {
            "location": "/feature-flags/#in-urls", 
            "text": "There are two ways to flag Django URL patterns in  urls.py : with  flagged_url()  in place of  url()  for a single pattern, or with the  flagged_urls()  context manager for multiple URLs.  flagged_url(flag_name, regex, view, kwargs=None, name=None, state=True, fallback=None)  works exactly like  url()  except it takes a flag name as its first argument. If the flag's state matches the given  state , the URL pattern will be served from the given  view ; if not, and  fallback  is given, the  fallback  will be used.  An example is  our  WAGTAIL_ABOUT_US  flag :  flagged_url('WAGTAIL_ABOUT_US',\n            r'^about-us/$',\n            lambda req: ServeView.as_view()(req, req.path),\n            fallback=SheerTemplateView.as_view(\n                template_name='about-us/index.html'),\n            name='about-us'),  Ignoring the  view  being a  lambda  for now (see  Flagging Wagtail URLs below ), this URL will be served via Wagtail if  WAGTAIL_ABOUT_US 's conditions are  True , and from a  TemplateView  if its conditions are  False .  If you need to flag multiple URLs with the same flag, you can use the  flagged_urls()  context manager.  with flagged_urls(flag_name, state=True, fallback=None) as url  provides a context in which the returned  url()  function can be used in place of the Django  url()  function in patterns and those patterns will share the same feature flag, state, and fallback.  An example is  our  WAGTAIL_ASK_CFPB  flag :  with flagged_urls('WAGTAIL_ASK_CFPB') as url:\n    ask_patterns = [\n        url(r'^(?i)ask-cfpb/([-\\w]{1,244})-(en)-(\\d{1,6})/?$',\n            view_answer,\n            name='ask-english-answer'),\n        url(r'^(?i)obtener-respuestas/([-\\w]{1,244})-(es)-(\\d{1,6})/?$',\n            view_answer,\n            name='ask-spanish-answer'),\n        \u2026\n    ]\n\nurlpatterns += ask_patterns   Warning  Do not attempt to use  flag_check  or any flag state-checking functions in  urls.py . Because they will be evaluated on import of  urls.py  they will attempt to access the Django FlagState model before it is ready and will error.", 
            "title": "In URLs"
        }, 
        {
            "location": "/feature-flags/#flagging-wagtail-urls", 
            "text": "Wagtail views in  flagged_url  with a Django view as fallback (or vice-versa) can be a bit awkward. Django views are typically called with  request  as the first argument, and Wagtail's  serve  view takes both the request and the path. To get around this, in  flagged_url  we typically use a  lambda  for the view:  lambda req: ServeView.as_view()(req, req.path)  This lambda takes the request and calls the  Wagtail-Sharing   ServeView  (which we're using in place of  wagtail.wagtailcore.views.serve ).", 
            "title": "Flagging Wagtail URLs"
        }, 
        {
            "location": "/feature-flags/#enabling-a-flag", 
            "text": "Feature flags are enabled based on a set of conditions that are given either in the Django settings files (in  cfgov/cfgov/settings/ ) or in the Django or Wagtail admin. Multiple conditions can be given, both in settings and in the admin, and if any condition is satisfied a flag is enabled.  A list of available conditions and how to use them is available in the Wagtail-Flags documentation .", 
            "title": "Enabling a flag"
        }, 
        {
            "location": "/feature-flags/#hard-coded-conditions", 
            "text": "Conditions that are defined in the Django settings are hard-coded, and require a change to files in cfgov-refresh, a new tagged release, and new deployment to change. These conditions should be used for flags that are relatively long-lasting and that can require a round-trip through the release and deployment process to change.  When  adding a flag  to the Django settings the flag's dictionary of conditions can contain a condition name and value that must be satisfied for the flag to be enabled. The nature of that value changes depending on the condition type.  See the Wagtail-Flags conditions documentation  for more on individual conditions.  There is a simple  boolean  condition that is either  True  or  False , and if it is  True  the flag is enabled and if it is  False  the flag is disabled. If we want to always turn the  BETA_NOTICE  flag on in settings with a  boolean  condition, that would look like this:  FLAGS = {\n    # Beta banner, seen on beta.consumerfinance.gov\n    # When enabled, a banner appears across the top of the site proclaiming\n    #  This beta site is a work in progress. \n    'BETA_NOTICE': {\n        'boolean': True,\n    },\n}", 
            "title": "Hard-coded conditions"
        }, 
        {
            "location": "/feature-flags/#database-conditions", 
            "text": "Conditions that are managed via the Wagtail or Django admin are stored in the database. These conditions can be changed in real-time and do not require any code changes or release and deployment to change (presuming the code that uses the feature flag is in place).  To view, delete, and add database conditions, navigate to \"Settings   Flags\" in the Wagtail admin.   Once in the flag settings, you'll have a list of all flags and their conditions..   Database conditions can be deleted with the trash can button on the right.  To create a new database condition, select \"Add a condition\". As with  hard-coded conditions , to create a database condition you must select which condition type you would like to use and give it a value that must be satisfied for the flag to be enabled.   Database conditions can only be set for flags that exist in the Django settings.", 
            "title": "Database conditions"
        }, 
        {
            "location": "/feature-flags/#satellite-apps", 
            "text": "Feature flags can be used in satellite apps in exactly the same way they are used in cfgov-refresh. An example is  the use of a feature flagged template choice in the complaintdatabase app .", 
            "title": "Satellite apps"
        }, 
        {
            "location": "/feature-flags/#hygiene", 
            "text": "Feature flags should be rare and ephemeral. Changes should be small and frequent, and not big-bang releases, and flags that are no longer used and their conditions should be cleaned up and removed from code and the database.", 
            "title": "Hygiene"
        }, 
        {
            "location": "/wagtail-migrations/", 
            "text": "Wagtail and Django data migrations\n\n\nDjango data migrations with Wagtail can be challenging because programmatic editing of Wagtail pages \nis difficult\n, and pages have both revisions and StreamFields. This document is intended to describe ways we try to address these challenges in cfgov-refresh.\n\n\nMigrating StreamFields\n\n\nStreamFields do not follow a fixed structure, rather they're a freeform sequences of blocks. Making a change to a StreamField involves both creating a \nDjango schema migration\n and a custom \nDjango data migration\n. The data migration needs to modify both the existing Wagtail pages that correspond to the changed model and all revisions of that page. It also needs to be able to manipulate the StreamField contents.\n\n\nTo this end, there are some utility functions in cfgov-refresh that make this easier. Using these utilities, a Django data migration that modifies a StreamField would use the following format:\n\n\nfrom django.db import migrations\n\nfrom v1.util.migrations import migrate_page_types_and_fields\n\n\ndef forward_mapper(page_or_revision, data):\n    data = dict(data)\n    # Manipulate the stream block data forwards\n    return data\n\n\ndef backward_mapper(page_or_revision, data):\n    data = dict(data)\n    # Manipulate the stream block data backwards\n    return data\n\n\ndef forwards(apps, schema_editor):\n    page_types_and_fields = [\n        ('myapp', 'MyPage', 'streamfield_name', 'streamblock_type'),\n    ]\n    migrate_page_types_and_fields(apps,\n                                  page_types_and_fields,\n                                  forward_mapper)\n\n\ndef backwards(apps, schema_editor):\n    page_types_and_fields = [\n        ('myapp', 'MyPage', 'streamfield_name', 'streamblock_type'),\n    ]\n    migrate_page_types_and_fields(apps,\n                                  page_types_and_fields,\n                                  backward_mapper)\n\n\nclass Migration(migrations.Migration):\n    dependencies = []\n    operations = [\n        migrations.RunPython(forwards, backwards),\n    ]\n\n\n\nUtility functions\n\n\nThese functions are available in \nv1.util.migrations\n.\n\n\nmigrate_page_types_and_fields(apps, page_types_and_fields, mapper)\n\n\nMigrate the fields of a wagtail page type using the given mapper function. page_types_and_fields should be a list of 4-tuples providing ('app', 'PageType', 'field_name', 'block type').\n\n\nThe mapper function should take \npage_or_revision\n and the stream block value.\n\n\nmigrate_stream_field(page_or_revision, field_name, block_type, mapper)\n\n\nMigrate a block of the type within a StreamField of the name belonging to the page or revision using the mapper function.\n\n\nThe mapper function should take \npage_or_revision\n and the stream block value.\n\n\nget_stream_data(page_or_revision, field_name)\n\n\nGet the stream field data for a given field name on a page or a revision.\n\n\nThis function will return a list of \ndict\n-like objects containing the blocks within the given StreamField.\n\n\nset_stream_data(page_or_revision, field_name, stream_data, commit=True)\n\n\nSet the stream field data for a given field name on a page or a revision. If commit is True (default) \nsave()\n is called on the \npage_or_revision\n object.\n\n\nstream_data\n must be a list of \ndict\n-like objects containing the blocks within the given StreamField.", 
            "title": "Wagtail and Django migrations"
        }, 
        {
            "location": "/wagtail-migrations/#wagtail-and-django-data-migrations", 
            "text": "Django data migrations with Wagtail can be challenging because programmatic editing of Wagtail pages  is difficult , and pages have both revisions and StreamFields. This document is intended to describe ways we try to address these challenges in cfgov-refresh.", 
            "title": "Wagtail and Django data migrations"
        }, 
        {
            "location": "/wagtail-migrations/#migrating-streamfields", 
            "text": "StreamFields do not follow a fixed structure, rather they're a freeform sequences of blocks. Making a change to a StreamField involves both creating a  Django schema migration  and a custom  Django data migration . The data migration needs to modify both the existing Wagtail pages that correspond to the changed model and all revisions of that page. It also needs to be able to manipulate the StreamField contents.  To this end, there are some utility functions in cfgov-refresh that make this easier. Using these utilities, a Django data migration that modifies a StreamField would use the following format:  from django.db import migrations\n\nfrom v1.util.migrations import migrate_page_types_and_fields\n\n\ndef forward_mapper(page_or_revision, data):\n    data = dict(data)\n    # Manipulate the stream block data forwards\n    return data\n\n\ndef backward_mapper(page_or_revision, data):\n    data = dict(data)\n    # Manipulate the stream block data backwards\n    return data\n\n\ndef forwards(apps, schema_editor):\n    page_types_and_fields = [\n        ('myapp', 'MyPage', 'streamfield_name', 'streamblock_type'),\n    ]\n    migrate_page_types_and_fields(apps,\n                                  page_types_and_fields,\n                                  forward_mapper)\n\n\ndef backwards(apps, schema_editor):\n    page_types_and_fields = [\n        ('myapp', 'MyPage', 'streamfield_name', 'streamblock_type'),\n    ]\n    migrate_page_types_and_fields(apps,\n                                  page_types_and_fields,\n                                  backward_mapper)\n\n\nclass Migration(migrations.Migration):\n    dependencies = []\n    operations = [\n        migrations.RunPython(forwards, backwards),\n    ]", 
            "title": "Migrating StreamFields"
        }, 
        {
            "location": "/wagtail-migrations/#utility-functions", 
            "text": "These functions are available in  v1.util.migrations .", 
            "title": "Utility functions"
        }, 
        {
            "location": "/wagtail-migrations/#migrate_page_types_and_fieldsapps-page_types_and_fields-mapper", 
            "text": "Migrate the fields of a wagtail page type using the given mapper function. page_types_and_fields should be a list of 4-tuples providing ('app', 'PageType', 'field_name', 'block type').  The mapper function should take  page_or_revision  and the stream block value.", 
            "title": "migrate_page_types_and_fields(apps, page_types_and_fields, mapper)"
        }, 
        {
            "location": "/wagtail-migrations/#migrate_stream_fieldpage_or_revision-field_name-block_type-mapper", 
            "text": "Migrate a block of the type within a StreamField of the name belonging to the page or revision using the mapper function.  The mapper function should take  page_or_revision  and the stream block value.", 
            "title": "migrate_stream_field(page_or_revision, field_name, block_type, mapper)"
        }, 
        {
            "location": "/wagtail-migrations/#get_stream_datapage_or_revision-field_name", 
            "text": "Get the stream field data for a given field name on a page or a revision.  This function will return a list of  dict -like objects containing the blocks within the given StreamField.", 
            "title": "get_stream_data(page_or_revision, field_name)"
        }, 
        {
            "location": "/wagtail-migrations/#set_stream_datapage_or_revision-field_name-stream_data-committrue", 
            "text": "Set the stream field data for a given field name on a page or a revision. If commit is True (default)  save()  is called on the  page_or_revision  object.  stream_data  must be a list of  dict -like objects containing the blocks within the given StreamField.", 
            "title": "set_stream_data(page_or_revision, field_name, stream_data, commit=True)"
        }, 
        {
            "location": "/django-to-wagtail/", 
            "text": "Wagtail pages vs. Django views\n\n\nRather than using \u201cclassic\u201d Django views added to urls.py, when feasible an app should provide singleton Wagtail Page. This will allow site editors to drop that page anywhere in the site\u2019s URL structure that they wish. A Wagtail Page subclass can do anything a Django view can \nwhen overriding the serve method\n.\n\n\nfrom django.http import HttpResponse\nfrom wagtail.wagtailcore.models import Page,\n\nclass HelloWorldPage(CFGOVPage):\n    def serve(self, request):\n        return HttpResponse(\nHello World\n)\n\n\n\nBy working with the Wagtail CMS, we also get some of the benefits of feature flags for free.", 
            "title": "Wagtail Pages vs Django views"
        }, 
        {
            "location": "/django-to-wagtail/#wagtail-pages-vs-django-views", 
            "text": "Rather than using \u201cclassic\u201d Django views added to urls.py, when feasible an app should provide singleton Wagtail Page. This will allow site editors to drop that page anywhere in the site\u2019s URL structure that they wish. A Wagtail Page subclass can do anything a Django view can  when overriding the serve method .  from django.http import HttpResponse\nfrom wagtail.wagtailcore.models import Page,\n\nclass HelloWorldPage(CFGOVPage):\n    def serve(self, request):\n        return HttpResponse( Hello World )  By working with the Wagtail CMS, we also get some of the benefits of feature flags for free.", 
            "title": "Wagtail pages vs. Django views"
        }, 
        {
            "location": "/testing-be/", 
            "text": "Backend testing\n\n\nDjango and Python unit tests\n\n\nTo run the the full suite of Python 2.7 unit tests using Tox, cd to the \nproject root, make sure the \nTOXENV\n variable is set in your \n.env\n file \nand then run:\n\n\ntox\n\n\n\nIf you haven't changed any installed packages and you don't need to test \nall migrations, you can run a much faster Python code test using:\n\ntox -e fast\n\n\nTo see Python code coverage information, run\n\n./show_coverage.sh\n\n\nSource code linting\n\n\nWe use the \nflake8\n and \nisort\n tools to ensure compliance with \n\nPEP8 style guide\n and the \n\nDjango coding style guidelines\n. \nWe do make two exceptions to PEP8 that are ignored in our flake8 \nconfiguration:\n\n\n\n\nE731\n, we allow assignment of lambda expressions\n\n\nW503\n, we allow line breaks after binary operators\n\n\n\n\nBoth \nflake8\n and \nisort\n can be run using the Tox \nlint\n environment:\n\n\ntox -e lint\n\n\n\nThis will run \nisort\n in check-only mode and it will print diffs for imports \nthat need to be fixed. To automatically fix import sort issues, run:\n\n\nisort --recursive cfgov/\n\n\n\nFrom the root of \ncfgov-refresh\n.", 
            "title": "Testing"
        }, 
        {
            "location": "/testing-be/#backend-testing", 
            "text": "", 
            "title": "Backend testing"
        }, 
        {
            "location": "/testing-be/#django-and-python-unit-tests", 
            "text": "To run the the full suite of Python 2.7 unit tests using Tox, cd to the \nproject root, make sure the  TOXENV  variable is set in your  .env  file \nand then run:  tox  If you haven't changed any installed packages and you don't need to test \nall migrations, you can run a much faster Python code test using: tox -e fast  To see Python code coverage information, run ./show_coverage.sh", 
            "title": "Django and Python unit tests"
        }, 
        {
            "location": "/testing-be/#source-code-linting", 
            "text": "We use the  flake8  and  isort  tools to ensure compliance with  PEP8 style guide  and the  Django coding style guidelines . \nWe do make two exceptions to PEP8 that are ignored in our flake8 \nconfiguration:   E731 , we allow assignment of lambda expressions  W503 , we allow line breaks after binary operators   Both  flake8  and  isort  can be run using the Tox  lint  environment:  tox -e lint  This will run  isort  in check-only mode and it will print diffs for imports \nthat need to be fixed. To automatically fix import sort issues, run:  isort --recursive cfgov/  From the root of  cfgov-refresh .", 
            "title": "Source code linting"
        }, 
        {
            "location": "/translation/", 
            "text": "Translation\n\n\nAs cfgov-refresh is a Django project, the \nDjango translation documentation is a good place to start\n. What follows is a brief introduction to translations with the particular tools cfgov-refresh uses (like Jinja2 templates) and the conventions we use.\n\n\nOverview\n\n\nTranslation is generally handled by one form or another of \ngettext\n. By convention, this is usually performed in code by wrapping a string to be translated in a function that is either named or aliased with an underscore. For example:\n\n\n_(\nThis is a translatable string.\n)\n\n\n\nThese strings are collected into portable object (\n.po\n) files for each supported language. These files map the original string (\nmsgid\n) to a matching translated string (\nmsgstr\n). For example:\n\n\nmsgid \nThis is a translatable string.\n\nmsgstr \nEsta es una cadena traducible.\n\n\n\nThese portable object files are compiled into machine object files (\n.mo\n) that the translation system uses when looking up the original string.\n\n\nBy convention the \n.po\n and \n.mo\n files live inside a \nlocale/[LANGUAGE]/LC_MESSAGES/\n folder structure, for example, \ncfgov/locale/es/LC_MESSAGES/django.po\n for the Spanish language portable object file for all of our cfgov-refresh messages.\n\n\nHow to translate text in cfgov-refresh\n\n\nThis brief howto will guide you through adding translatable text to cfgov-refresh.\n\n\n1. Add the translation function around the string\n\n\nIn Jinja2 templates:\n\n\n{{ _('Hello World!') }}\n\n\n\nIn Django templates:\n\n\n{% load i18n %}\n\n{% trans \nHello World!\n %}\n\n\n\nIn Python code:\n\n\nfrom django.utils.translation import ugettext as _\n\nmystring = _('Hello World!')\n\n\n\nThe string in the call to the translation function will be the \nmsgid\n in the portable object file below.\n\n\n2. Run the \nmakemessages\n management command to add the string to the portable object file\n\n\nThe \nmakemessages\n management command will look through all Python, Django, and Jinja2 template files to find strings that are wrapped in a translation function call and add them to the portable object file for a particular language. The language is specified with \n-l\n. The command also must be called from the root of the Django app tree, \nnot\n the project root.\n\n\nTo generate or update the portable object file for Spanish:\n\n\ncd cfgov\ndjango-admin.py makemessages -l es\n\n\n\n3. Edit the portable object file to add a translation for the string\n\n\nThe portable object files are stored in \ncfgov/locale/[LANGUAGE]/LC_MESSAGES/\n. For the Spanish portable object file, edit \ncfgov/locale/es/LC_MESSAGES/django.po\n and add the Spanish translation as the \nmsgstr\n for your new \nmsgid\n\n\nmsgid \nHello World!\n\nmsgstr \nHola Mundo!\n\n\n\n4. Run the \ncompilemessages\n management command to compile the machine object file\n\n\ncd cfgov\ndjango-admin.py compilemessages\n\n\n\nWagtail Considerations\n\n\nAll of our Wagtail pages include a language-selection dropdown under its Configuration tab:\n\n\n\n\nThe selected language will force translation of all translatable strings in templates and code for that page.", 
            "title": "Translation"
        }, 
        {
            "location": "/translation/#translation", 
            "text": "As cfgov-refresh is a Django project, the  Django translation documentation is a good place to start . What follows is a brief introduction to translations with the particular tools cfgov-refresh uses (like Jinja2 templates) and the conventions we use.", 
            "title": "Translation"
        }, 
        {
            "location": "/translation/#overview", 
            "text": "Translation is generally handled by one form or another of  gettext . By convention, this is usually performed in code by wrapping a string to be translated in a function that is either named or aliased with an underscore. For example:  _( This is a translatable string. )  These strings are collected into portable object ( .po ) files for each supported language. These files map the original string ( msgid ) to a matching translated string ( msgstr ). For example:  msgid  This is a translatable string. \nmsgstr  Esta es una cadena traducible.  These portable object files are compiled into machine object files ( .mo ) that the translation system uses when looking up the original string.  By convention the  .po  and  .mo  files live inside a  locale/[LANGUAGE]/LC_MESSAGES/  folder structure, for example,  cfgov/locale/es/LC_MESSAGES/django.po  for the Spanish language portable object file for all of our cfgov-refresh messages.", 
            "title": "Overview"
        }, 
        {
            "location": "/translation/#how-to-translate-text-in-cfgov-refresh", 
            "text": "This brief howto will guide you through adding translatable text to cfgov-refresh.", 
            "title": "How to translate text in cfgov-refresh"
        }, 
        {
            "location": "/translation/#1-add-the-translation-function-around-the-string", 
            "text": "In Jinja2 templates:  {{ _('Hello World!') }}  In Django templates:  {% load i18n %}\n\n{% trans  Hello World!  %}  In Python code:  from django.utils.translation import ugettext as _\n\nmystring = _('Hello World!')  The string in the call to the translation function will be the  msgid  in the portable object file below.", 
            "title": "1. Add the translation function around the string"
        }, 
        {
            "location": "/translation/#2-run-the-makemessages-management-command-to-add-the-string-to-the-portable-object-file", 
            "text": "The  makemessages  management command will look through all Python, Django, and Jinja2 template files to find strings that are wrapped in a translation function call and add them to the portable object file for a particular language. The language is specified with  -l . The command also must be called from the root of the Django app tree,  not  the project root.  To generate or update the portable object file for Spanish:  cd cfgov\ndjango-admin.py makemessages -l es", 
            "title": "2. Run the makemessages management command to add the string to the portable object file"
        }, 
        {
            "location": "/translation/#3-edit-the-portable-object-file-to-add-a-translation-for-the-string", 
            "text": "The portable object files are stored in  cfgov/locale/[LANGUAGE]/LC_MESSAGES/ . For the Spanish portable object file, edit  cfgov/locale/es/LC_MESSAGES/django.po  and add the Spanish translation as the  msgstr  for your new  msgid  msgid  Hello World! \nmsgstr  Hola Mundo!", 
            "title": "3. Edit the portable object file to add a translation for the string"
        }, 
        {
            "location": "/translation/#4-run-the-compilemessages-management-command-to-compile-the-machine-object-file", 
            "text": "cd cfgov\ndjango-admin.py compilemessages", 
            "title": "4. Run the compilemessages management command to compile the machine object file"
        }, 
        {
            "location": "/translation/#wagtail-considerations", 
            "text": "All of our Wagtail pages include a language-selection dropdown under its Configuration tab:   The selected language will force translation of all translatable strings in templates and code for that page.", 
            "title": "Wagtail Considerations"
        }, 
        {
            "location": "/caching/", 
            "text": "Caching\n\n\nAkamai\n\n\nWe use \nAkamai\n, a content delivery network, to cache the entirety of \nwww.consumerfinance.gov\n (but not our development servers). We invalidate any given page in Wagtail when it is published or unpublished (by hooking up the custom class \nAkamaiBackend\n to \nWagtail's frontend cache invalidator\n. By default, we clear the Akamai cache any time we deploy. \n\n\nThere are certain pages that do not live in Wagtail or are impacted by changes on another page (imagine our \nnewsroom page\n that lists titles of other pages) or another process (imagine data from Socrata gets updated) and thus will display outdated content until the page's time to live (TTL) has expired, a deploy has happened, or if someone manually invalidates that page. Our default TTL is 24 hours.\n\n\nDjango caching\n\n\nStarting in December 2017, we use \ntemplate fragment caching\n to cache all or part of a template.  It is enabled on our \"post previews\", snippets of a page that we display on results of filterable pages (e.g. \nour blog page\n \n \nresearch \n reports\n).\n\n\nIt can easily be enabled on other templates. See \nthis PR\n as an example of the code that would need to be introduced to cache a new fragment. \n\n\nWhen a page gets published, it will update the post preview cache for that particular page.  However, if there are code changes that impact the page's content, or the post preview template itself gets updated, the entire post preview cache will need to be manually cleared. Clearing this particular cache could be an option when deploying, as it is with Akamai, but should not be a default since most deploys wouldn't impact the code in question.  Currently, the manual way to do this would be to run the following from a production server:\n\n\nfrom django.core.cache import caches\n\ncaches['post_preview'].clear()\n\n\n\nTo run the application locally with caching for post previews enabled, run \nENABLE_POST_PREVIEW_CACHE=1 ./runserver.sh\n\nAlternatively, add this variable to your \n.env\n if you generally want it enabled locally.\n\n\nDue to the impossibility/difficulty/complexity of caching individual Wagtail blocks (they are not serializable) and invalidating content that does not have some type of \npost_save\n hook (e.g. Taggit models), we have started with caching segments that are tied to a Wagtail page (which can be easily invalidated using the \npage_published\n Wagtail signal), hence the post previews. With more research or improvements to these third-party libraries, it is possible we could expand Django-level caching to more content.", 
            "title": "Caching"
        }, 
        {
            "location": "/caching/#caching", 
            "text": "", 
            "title": "Caching"
        }, 
        {
            "location": "/caching/#akamai", 
            "text": "We use  Akamai , a content delivery network, to cache the entirety of  www.consumerfinance.gov  (but not our development servers). We invalidate any given page in Wagtail when it is published or unpublished (by hooking up the custom class  AkamaiBackend  to  Wagtail's frontend cache invalidator . By default, we clear the Akamai cache any time we deploy.   There are certain pages that do not live in Wagtail or are impacted by changes on another page (imagine our  newsroom page  that lists titles of other pages) or another process (imagine data from Socrata gets updated) and thus will display outdated content until the page's time to live (TTL) has expired, a deploy has happened, or if someone manually invalidates that page. Our default TTL is 24 hours.", 
            "title": "Akamai"
        }, 
        {
            "location": "/caching/#django-caching", 
            "text": "Starting in December 2017, we use  template fragment caching  to cache all or part of a template.  It is enabled on our \"post previews\", snippets of a page that we display on results of filterable pages (e.g.  our blog page     research   reports ).  It can easily be enabled on other templates. See  this PR  as an example of the code that would need to be introduced to cache a new fragment.   When a page gets published, it will update the post preview cache for that particular page.  However, if there are code changes that impact the page's content, or the post preview template itself gets updated, the entire post preview cache will need to be manually cleared. Clearing this particular cache could be an option when deploying, as it is with Akamai, but should not be a default since most deploys wouldn't impact the code in question.  Currently, the manual way to do this would be to run the following from a production server:  from django.core.cache import caches\n\ncaches['post_preview'].clear()  To run the application locally with caching for post previews enabled, run  ENABLE_POST_PREVIEW_CACHE=1 ./runserver.sh \nAlternatively, add this variable to your  .env  if you generally want it enabled locally.  Due to the impossibility/difficulty/complexity of caching individual Wagtail blocks (they are not serializable) and invalidating content that does not have some type of  post_save  hook (e.g. Taggit models), we have started with caching segments that are tied to a Wagtail page (which can be easily invalidated using the  page_published  Wagtail signal), hence the post previews. With more research or improvements to these third-party libraries, it is possible we could expand Django-level caching to more content.", 
            "title": "Django caching"
        }, 
        {
            "location": "/atomic-structure/", 
            "text": "Notes on Atomic Design\n\n\nCheck out \nDon't Build Pages, Build Modules\n.\nIt encompasses exactly what we are trying to achieve by building components\nusing atomic design.\nIt's important to note that our front-end atomic architecture is still evolving.\n\n\nOur components are broken down into templates, organisms, molecules, and atoms.\nWe opted not to use the page component, although it exists in atomic design.\nOur components are composed of HTML, CSS, and JS (JavaScript).\nIf a component doesn\u2019t have user interactions or require styling,\nthen it won\u2019t have an associated JS and/or CSS file.\n\n\nWe compose our atomic components as follows:\n\n\nAtoms\n\n\nPrefixed with \u201ca-\u201d in CSS, JavaScript, and HTML files.\n\n\nHTML\n\n\ndiv class=\na-overlay u-hidden\n/div\n\n\n\nCSS\n\n\n .a-overlay {\n    // Only show overlay at mobile/tablet size.\n    .respond-to-max( @bp-sm-max, {\n        height: 100%;\n        width: 100%;\n \u2026\n\n\n\nMolecules\n\n\nPrefixed with \u201cm-\u201d in CSS, JavaScript, and HTML files.\n\n\nHTML\n\n\ndiv class=\nm-notification\n            m-notification__visible\n            m-notification__error\n\n     data-js-hook=\nstate_atomic_init\n\n    \nspan class=\nm-notification_icon cf-icon\n/span\n\n    \ndiv class=\nm-notification_content\n role=\nalert\n\n        \ndiv class=\nh4 m-notification_message\nPage not found.\n/div\n\n    \n/div\n\n\n/div\n\n\n\nCSS\n\n\n.m-notification {\n    display: none;\n    position: relative;\n    padding: @notification-padding__px;\n    \u2026\n\n\n\nJavaScript\n\n\nfunction Notification( element ) {\n   const BASE_CLASS = 'm-notification';\n\n   // Constants for the state of this Notification.\n   const SUCCESS = 'success';\n   const WARNING = 'warning';\n   const ERROR = 'error';\n\n   // Constants for the Notification modifiers.\n   const MODIFIER_VISIBLE = BASE_CLASS + '__visible';\n   const _dom = atomicHelpers.checkDom( element, BASE_CLASS );\n   const _contentDom = _dom.querySelector( '.' + BASE_CLASS + '_content' );\n   \u2026\n\n\n\nThe notification molecule can be instantiated with the following code:\n\n\nconst notification = new Notification( _dom );\nnotification.init();\n\n\n\nOrganisms\n\n\nPrefixed with \u201co-\u201d in CSS, JavaScript, and HTML.\n\n\nHTML\n\n\ndiv class=\no-expandable\n            o-expandable__borders\n            o-expandable__midtone\n            o-expandable__expanded\n\n     data-js-hook=\nstate_atomic_init\n\n    \nbutton class=\no-expandable_target\n aria-pressed=\ntrue\n\n        \ndiv class=\no-expandable_header\n\n        \u2026\n\n\n\nJavaScript:\n\n\n function Expandable( element ) {\n  const BASE_CLASS = 'o-expandable';\n\n  // Bitwise flags for the state of this Expandable.\n  const COLLAPSED = 0;\n  const COLLAPSING = 1;\n  const EXPANDING = 2;\n  const EXPANDED = 3;\n\n  // The Expandable element will directly be the Expandable\n  // when used in an ExpandableGroup, otherwise it can be the parent container.\n  const _dom = atomicHelpers.checkDom( element, BASE_CLASS );\n  const _target = _dom.querySelector( '.' + BASE_CLASS + '_target' );\n  const _content = _dom.querySelector( '.' + BASE_CLASS + '_content' );\n  \u2026\n\n\n\nThe Expandable organism can be instantiated with the following code:\n\n\nconst expandable = new Expandable( _dom.querySelector( '.o-expandable' ) );\nexpandable.init( _expandable.EXPANDED );\n\n\n\nor\n\n\nconst atomicHelpers = require( '../../modules/util/atomic-helpers' );\nconst Expandable = require( '../../organisms/Expandable' );\natomicHelpers.instantiateAll( '.o-expandable', Expandable );\n\n\n\nTemplates\n\n\nPrefixed with \u201ct-\u201d in CSS, JavaScript, and HTML. \nView all available templates\n that can be extended or reused to create pages.\n\n\nCSS\n\n\n.t-careers {\n    \n_social .m-social-media {\n        float: right;\n    }\n    \u2026\n\n\n\nFolder structure\n\n\nOur atomic components are separated and named based on asset type. HTML, CSS, and JavaScript for each component are in separate directories.\n\n\nCurrent structure\n\n\nHTML\n\n\ncfgov-refresh/cfgov/jinja2/v1/_includes/atoms/\ncfgov-refresh/cfgov/jinja2/v1/_includes/molecules/\ncfgov-refresh/cfgov/jinja2/v1/_includes/organisms/\n\n\n\nCSS\n\n\ncfgov-refresh/cfgov/unprocessed/css/atoms/\ncfgov-refresh/cfgov/unprocessed/css/molecules/\ncfgov-refresh/cfgov/unprocessed/css/organisms/\n\n\n\nJavaScript\n\n\ncfgov-refresh/cfgov/unprocessed/js/atoms/\ncfgov-refresh/cfgov/unprocessed/js/molecules/\ncfgov-refresh/cfgov/unprocessed/js/organisms/\n\n\n\nTest\n\n\ncfgov-refresh/test/unit_tests/atoms/\ncfgov-refresh/test/unit_tests/molecules/\ncfgov-refresh/test/unit_tests/organisms/\n\n\n\nJavaScript architecture\n\n\nJavaScript components are built to be rendered on the server and then enhanced via JavaScript on the client. The basic interface for the components is as follows:\n\n\nfunction AtomicComponent( domElement ) {\n    // Ensure the passed in Element is in the DOM.\n    // Query and store references to sub-elements.\n    // Instantiate child atomic components.\n    // Bind necessary events for referenced DOM elements.\n    // Perform other initialization related tasks.\n    this.init = function init(){}\n\n    // General teardown function\n    // We don't remove the element from the DOM so\n    // we need to unbind the events.\n    this.destroy = function destroy(){}\n}\n\n\n\nWe generally favor composition over inheritance.\nYou can get more information by reading the following:\n\n\nArticles\n\n\nA Simple Challenge to Classical Inheritance Fans\n\n\nComposition over Inheritance (Youtube)\n\n\nComponent build pipeline\n\n\nGulp\n\n\nGulp is used as a task automation tool. Tasks include compiling CSS, creating a standard webpack workflow for bundling scripts, minifying code, linting, image optimizing, running unit tests, and \nmore\n.\n\n\nWebpack\n\n\nWepback is used as a module bundler although it's capable of more.\nWe create page, global, and atomic specific bundles.\nThe configuration for the bundles is contained in config/webpack-config.js.\nAn explanation for the usage of each bundle is contained in scripts.js.\n\n\nRoutes\n\n\nRoutes are used to serve JavaScript bundles to the browser based\non the requested URL or Wagtail page Media property.\nThis happens via code contained in \nbase.html\n. This file serves as the base HTML template for serving up assets and content. \nView base.html on Github\n.\n\n\nWagtail page media property\n\n\nEach atomic component has a media property that list the JavaScript files\nthat should be rendered via base.html.\nWhen a page is requested via the browser, code contained in base.html will\nloop all atomic components for the requested page and render\nthe appropriate atomic JavaScript bundles.\n\n\nHere is an example of the media property on a component from the \nEmail signup organism\n:\n\n\nclass Media:\n    js = ['email-signup.js']\n\n\n\nThis will load the \nemail-signup.js\n script on any page that includes the Email Signup organism in its template.", 
            "title": "Atomic structure and design"
        }, 
        {
            "location": "/atomic-structure/#notes-on-atomic-design", 
            "text": "Check out  Don't Build Pages, Build Modules .\nIt encompasses exactly what we are trying to achieve by building components\nusing atomic design.\nIt's important to note that our front-end atomic architecture is still evolving.  Our components are broken down into templates, organisms, molecules, and atoms.\nWe opted not to use the page component, although it exists in atomic design.\nOur components are composed of HTML, CSS, and JS (JavaScript).\nIf a component doesn\u2019t have user interactions or require styling,\nthen it won\u2019t have an associated JS and/or CSS file.  We compose our atomic components as follows:", 
            "title": "Notes on Atomic Design"
        }, 
        {
            "location": "/atomic-structure/#atoms", 
            "text": "Prefixed with \u201ca-\u201d in CSS, JavaScript, and HTML files.", 
            "title": "Atoms"
        }, 
        {
            "location": "/atomic-structure/#html", 
            "text": "div class= a-overlay u-hidden /div", 
            "title": "HTML"
        }, 
        {
            "location": "/atomic-structure/#css", 
            "text": ".a-overlay {\n    // Only show overlay at mobile/tablet size.\n    .respond-to-max( @bp-sm-max, {\n        height: 100%;\n        width: 100%;\n \u2026", 
            "title": "CSS"
        }, 
        {
            "location": "/atomic-structure/#molecules", 
            "text": "Prefixed with \u201cm-\u201d in CSS, JavaScript, and HTML files.", 
            "title": "Molecules"
        }, 
        {
            "location": "/atomic-structure/#html_1", 
            "text": "div class= m-notification\n            m-notification__visible\n            m-notification__error \n     data-js-hook= state_atomic_init \n     span class= m-notification_icon cf-icon /span \n     div class= m-notification_content  role= alert \n         div class= h4 m-notification_message Page not found. /div \n     /div  /div", 
            "title": "HTML"
        }, 
        {
            "location": "/atomic-structure/#css_1", 
            "text": ".m-notification {\n    display: none;\n    position: relative;\n    padding: @notification-padding__px;\n    \u2026", 
            "title": "CSS"
        }, 
        {
            "location": "/atomic-structure/#javascript", 
            "text": "function Notification( element ) {\n   const BASE_CLASS = 'm-notification';\n\n   // Constants for the state of this Notification.\n   const SUCCESS = 'success';\n   const WARNING = 'warning';\n   const ERROR = 'error';\n\n   // Constants for the Notification modifiers.\n   const MODIFIER_VISIBLE = BASE_CLASS + '__visible';\n   const _dom = atomicHelpers.checkDom( element, BASE_CLASS );\n   const _contentDom = _dom.querySelector( '.' + BASE_CLASS + '_content' );\n   \u2026  The notification molecule can be instantiated with the following code:  const notification = new Notification( _dom );\nnotification.init();", 
            "title": "JavaScript"
        }, 
        {
            "location": "/atomic-structure/#organisms", 
            "text": "Prefixed with \u201co-\u201d in CSS, JavaScript, and HTML.", 
            "title": "Organisms"
        }, 
        {
            "location": "/atomic-structure/#html_2", 
            "text": "div class= o-expandable\n            o-expandable__borders\n            o-expandable__midtone\n            o-expandable__expanded \n     data-js-hook= state_atomic_init \n     button class= o-expandable_target  aria-pressed= true \n         div class= o-expandable_header \n        \u2026  JavaScript:   function Expandable( element ) {\n  const BASE_CLASS = 'o-expandable';\n\n  // Bitwise flags for the state of this Expandable.\n  const COLLAPSED = 0;\n  const COLLAPSING = 1;\n  const EXPANDING = 2;\n  const EXPANDED = 3;\n\n  // The Expandable element will directly be the Expandable\n  // when used in an ExpandableGroup, otherwise it can be the parent container.\n  const _dom = atomicHelpers.checkDom( element, BASE_CLASS );\n  const _target = _dom.querySelector( '.' + BASE_CLASS + '_target' );\n  const _content = _dom.querySelector( '.' + BASE_CLASS + '_content' );\n  \u2026  The Expandable organism can be instantiated with the following code:  const expandable = new Expandable( _dom.querySelector( '.o-expandable' ) );\nexpandable.init( _expandable.EXPANDED );  or  const atomicHelpers = require( '../../modules/util/atomic-helpers' );\nconst Expandable = require( '../../organisms/Expandable' );\natomicHelpers.instantiateAll( '.o-expandable', Expandable );", 
            "title": "HTML"
        }, 
        {
            "location": "/atomic-structure/#templates", 
            "text": "Prefixed with \u201ct-\u201d in CSS, JavaScript, and HTML.  View all available templates  that can be extended or reused to create pages.", 
            "title": "Templates"
        }, 
        {
            "location": "/atomic-structure/#css_2", 
            "text": ".t-careers {\n     _social .m-social-media {\n        float: right;\n    }\n    \u2026", 
            "title": "CSS"
        }, 
        {
            "location": "/atomic-structure/#folder-structure", 
            "text": "Our atomic components are separated and named based on asset type. HTML, CSS, and JavaScript for each component are in separate directories.", 
            "title": "Folder structure"
        }, 
        {
            "location": "/atomic-structure/#current-structure", 
            "text": "", 
            "title": "Current structure"
        }, 
        {
            "location": "/atomic-structure/#html_3", 
            "text": "cfgov-refresh/cfgov/jinja2/v1/_includes/atoms/\ncfgov-refresh/cfgov/jinja2/v1/_includes/molecules/\ncfgov-refresh/cfgov/jinja2/v1/_includes/organisms/", 
            "title": "HTML"
        }, 
        {
            "location": "/atomic-structure/#css_3", 
            "text": "cfgov-refresh/cfgov/unprocessed/css/atoms/\ncfgov-refresh/cfgov/unprocessed/css/molecules/\ncfgov-refresh/cfgov/unprocessed/css/organisms/", 
            "title": "CSS"
        }, 
        {
            "location": "/atomic-structure/#javascript_1", 
            "text": "cfgov-refresh/cfgov/unprocessed/js/atoms/\ncfgov-refresh/cfgov/unprocessed/js/molecules/\ncfgov-refresh/cfgov/unprocessed/js/organisms/", 
            "title": "JavaScript"
        }, 
        {
            "location": "/atomic-structure/#test", 
            "text": "cfgov-refresh/test/unit_tests/atoms/\ncfgov-refresh/test/unit_tests/molecules/\ncfgov-refresh/test/unit_tests/organisms/", 
            "title": "Test"
        }, 
        {
            "location": "/atomic-structure/#javascript-architecture", 
            "text": "JavaScript components are built to be rendered on the server and then enhanced via JavaScript on the client. The basic interface for the components is as follows:  function AtomicComponent( domElement ) {\n    // Ensure the passed in Element is in the DOM.\n    // Query and store references to sub-elements.\n    // Instantiate child atomic components.\n    // Bind necessary events for referenced DOM elements.\n    // Perform other initialization related tasks.\n    this.init = function init(){}\n\n    // General teardown function\n    // We don't remove the element from the DOM so\n    // we need to unbind the events.\n    this.destroy = function destroy(){}\n}  We generally favor composition over inheritance.\nYou can get more information by reading the following:", 
            "title": "JavaScript architecture"
        }, 
        {
            "location": "/atomic-structure/#articles", 
            "text": "A Simple Challenge to Classical Inheritance Fans  Composition over Inheritance (Youtube)", 
            "title": "Articles"
        }, 
        {
            "location": "/atomic-structure/#component-build-pipeline", 
            "text": "", 
            "title": "Component build pipeline"
        }, 
        {
            "location": "/atomic-structure/#gulp", 
            "text": "Gulp is used as a task automation tool. Tasks include compiling CSS, creating a standard webpack workflow for bundling scripts, minifying code, linting, image optimizing, running unit tests, and  more .", 
            "title": "Gulp"
        }, 
        {
            "location": "/atomic-structure/#webpack", 
            "text": "Wepback is used as a module bundler although it's capable of more.\nWe create page, global, and atomic specific bundles.\nThe configuration for the bundles is contained in config/webpack-config.js.\nAn explanation for the usage of each bundle is contained in scripts.js.", 
            "title": "Webpack"
        }, 
        {
            "location": "/atomic-structure/#routes", 
            "text": "Routes are used to serve JavaScript bundles to the browser based\non the requested URL or Wagtail page Media property.\nThis happens via code contained in  base.html . This file serves as the base HTML template for serving up assets and content.  View base.html on Github .", 
            "title": "Routes"
        }, 
        {
            "location": "/atomic-structure/#wagtail-page-media-property", 
            "text": "Each atomic component has a media property that list the JavaScript files\nthat should be rendered via base.html.\nWhen a page is requested via the browser, code contained in base.html will\nloop all atomic components for the requested page and render\nthe appropriate atomic JavaScript bundles.  Here is an example of the media property on a component from the  Email signup organism :  class Media:\n    js = ['email-signup.js']  This will load the  email-signup.js  script on any page that includes the Email Signup organism in its template.", 
            "title": "Wagtail page media property"
        }, 
        {
            "location": "/testing-fe/", 
            "text": "Browser tests\n\n\nQuick start:\n\n\nTo run browser tests, open a new Terminal window or tab and change to the project directory,\nthen tell gulp to start the tests:\n\n\ngulp build\ngulp test:acceptance ( tox -e acceptance can be run as well )\n\n\n\nThere are several options you can pass to run a particular suite of tests,\nto run a particular list of features,\nand/or to run it in \"fast\" mode:\n\n\ngulp test:acceptance --suite=wagtail-admin ( runs just the wagtail-admin suite )\ngulp test:acceptance --specs=multi-select.feature ( runs just the multi-select feature )\ngulp test:acceptance --tags=@mobile ( runs all scenarios tagged with @mobile )\ngulp test:acceptance --fast ( runs the tests without recreating the virtual environment )\n\n\n\nThe same options can be used with tox (--omitted):\n\n\ntox -e acceptance suite=wagtail-admin\ntox -e acceptance specs=multi-select.feature\ntox -e acceptance tags=@mobile\ntox -e acceptance-fast\n\n\n\nThese tests will run on their own server; you do not need to be running your development server.\n\n\nCucumber - tool for running automated tests written in plain language\n\n\nBelow are some suggested standards for Cucumber Feature files:\n\n\nTable copied from \nhttps://saucelabs.com/blog/write-great-cucumber-tests\n by Greg Sypolt, with moderate modifications\n\n\n\n   \n\n      \n\n         \nFeature Files\n\n         \nEvery *.feature file consists in a single feature, focused on the business value.\n\n      \n\n      \n\n      \nGherkin\n\n         \n\n            \nFeature:Title (one line describing the story)\nNarrative Description: As a [role], I want [feature], so that I [benefit]\n\nScenario: Title (acceptance criteria of user story)\n  Given [context]\n  And [some more context]...\n  When [event]\n  Then [outcome]\n  And [another outcome]...\n\nScenario:...\n\n\n\n\n      \n\n      \n\n         \nGiven, When, and Then Statements\n\n         \n\n           There might be some confusion surrounding where to put the verification step in the Given, When, Then sequence. Each statement has a purpose. \n\n\n\n\nGiven\n is the pre-condition to put the system into a known state before the user starts interacting with the application\n\n\nWhen\n describes the key action the user performs\n\n\nThen\n is observing the expected outcome\n\n\n\n\nJust remember the \n\u2018then\u2019\n step is an acceptance criteria of the story.\n   \n\n      \n\n      \n\n         \nBackground\n\n         \nThe background needs to be used wisely. If you use the same steps at the beginning of all scenarios of a feature, put them into the feature\u2019s background scenario. The background steps are run before each scenario.\n\n\nBackground:\n  Given I am logged into Wagtail as an admin\n  And I create a Wagtail Sublanding Page\n  And I open the content menu\n\n        \n\n      \n\n      \n\n         \nScenarios\n\n         \nKeep each scenario independent. The scenarios should run independently, without any dependencies on other scenarios.  Scenarios should be between 3 to 6 statements, if possible.\n\n      \n\n      \n\n         \nScenario Outline\n\n         \nIf you identify the need to use a scenario outline, take a step back and ask the following question: Is it necessary to repeat this scenario \u2018x\u2019 amount of times just to exercise the different combination of data? In most cases, one time is enough for UI level testing.\n\n      \n\n      \n\n         \nDeclarative Vs Imperative Scenarios\n\n         \n\n            The declarative style describes behavior at a higher level, which improves the readability of the feature by abstracting out the implementation details of the application.  The imperative style is more verbose but better describes the expected behavior.  Either style is acceptable.\n\n\nExample: Declarative\n\n\n\nScenario:User logs in\n  Given I am on the homepage\n  When I log in\n  Then I should see a login notification\n\n\n\nExample: Imperative\n\n\n\nScenario: User logs in\n  Given I am on the homepage\n  When I click on the \"Login\" button\n  And I fill in the \"Email\" field with \"\n\"\n  And I fill in the \"Password\" field with \"secret\"\n  And I click on \"Submit\"\n  Then I should see \"Welcome to the app, John Doe\"\n\n\n         \n\n      \n\n   \n\n\n\n\nSauce Connect - send tests to the cloud\n\n\nSauce Labs can be used to run tests remotely in the cloud.\n\n\n\n\n\n\nLog into \nhttp://saucelabs.com/account\n.\n\n\n\n\n\n\nUpdate and uncomment the \nSAUCE_USERNAME\n, \nSAUCE_ACCESS_KEY\n,\n   and \nSAUCE_SELENIUM_URL\n values in your \n.env\n file.\n   The access key can be found on the Sauce Labs\n   \nuser settings page\n.\n\n\n\n\n\n\nReload the settings with \nsource .env\n.\n\n\n\n\n\n\nRun the tests with \ngulp test:acceptance --sauce\n.\n\n\n\n\n\n\nMonitor progress of the tests\n   on the \nSauce Labs dashboard\n Automated Tests tab.\n\n\n\n\n\n\n\n\nNote\n\n\nIf you get the error \nError: ENOTFOUND getaddrinfo ENOTFOUND\n\nwhile running a test, it likely means that Sauce Connect is not running.\n\n\n\n\nManual test configuration\n\n\nA number of command-line arguments can be set to test particular configurations:\n\n\n\n\n--suite\n: Choose a particular suite or suites to run.\n   For example, \ngulp test:acceptance --suite=content\n or \ngulp test:acceptance --suite=content,functional\n.\n\n\n--specs\n: Choose a particular spec or specs to run.\n   For example, \ngulp test:acceptance --specs=header.feature\n, \ngulp test:acceptance --specs=header.feature,pagination.feature\n, or \ngulp test:acceptance --specs=filterable*.feature\n. If \n--suite\n is specified, this argument will be ignored. If neither \n--suite\n nor \n--specs\n are specified, all specs will be run.\n\n\n--windowSize\n: Set the window size in pixels in \nw,h\n format.\n   For example, \ngulp test:acceptance --windowSize=900,400\n.\n\n\n--browserName\n: Set the browser to run.\n   For example, \ngulp test:acceptance --browserName=firefox\n.\n\n\n--version\n: Set the browser version to run.\n   For example, \ngulp test:acceptance --version='44.0'\n.\n\n\n--platform\n: Set the OS platform to run.\n   For example, \ngulp test:acceptance --platform='osx 10.10'\n.\n\n\n--sauce\n: Whether to run on Sauce Labs or not.\n   For example, \ngulp test:acceptance --sauce=false\n.\n\n\n\n\nTests\n\n\nTests are organized into suites under the \ntest/browser_tests/cucumber/features\n directory. Any new tests should be added to an existing suite (e.g. \"default\"), or placed into a new suite directory. All tests start with writing a \n.feature\n spec in one of these suites, and then adding corresponding step definitions, found in \ntest/browser_tests/cucumber/step_definitions\n.\n\n\nFurther reading\n\n\n\n\nCucumber features\n\n\nProtractor\n\n\nSelect elements on a page\n\n\nWriting Jasmin expectations\n.\n\n\nUnderstanding Page Objects\n\n\n\n\nPerformance testing\n\n\nTo audit if the site complies with performance best practices and guidelines,\nrun \ngulp audit:perf\n.\n\n\nThe audit will run against\n\nGoogle's PageSpeed Insights\n.\n\n\nUnit testing\n\n\nJavaScript unit tests\n\n\nJavaScript module unit tests are run with \ngulp test:unit\n.\n\n\nIf you want to run individual spec files, pass in the \n--specs\n command-line\nargument with the path to the spec,\nsuch as \ngulp test:unit --specs=modules/Tree-spec.js\n.\nGlobs can be used to run a group of unit tests in the same directory,\nsuch as \ngulp test:unit --specs=modules/transition/*.js\n.\n\n\nAccessibility Testing\n\n\nRun the acceptance tests with an \n--a11y\n flag (i.e. \ngulp test:acceptance --a11y\n)\nto check every webpage for WCAG and Section 508 compliancy using Protractor's\n\naccessibility plugin\n.\n\n\nIf you'd like to audit a specific page, use \ngulp audit:a11y\n:\n\n\n\n\nEnable the environment variable \nACHECKER_ID\n in your \n.env\n file.\n     Get a free \nAChecker API ID\n for the value.\n\n\nReload your \n.env\n with \nsource ./.env\n while in the project root directory.\n\n\nRun \ngulp test:a11y\n to run an audit on the homepage.\n\n\nTo test a page aside from the homepage, add the \n--u=\npath_to_test\n flag.\n     For example, \ngulp test:a11y --u=contact-us\n\n     or \ngulp test:a11y --u=the-bureau/bureau-structure/\n.\n\n\n\n\nSource code linting\n\n\nThe default test task includes linting of the JavaScript source, build,\nand test files.\nUse the \ngulp lint\n command from the command-line to run the ESLint linter,\nwhich checks the JavaScript against the rules configured in \n.eslintrc\n.\n\nSee the ESLint docs\n\nfor detailed rule descriptions.\n\n\nThere are a number of options to the command:\n\n\n\n\ngulp lint:build\n: Lint only the gulp build scripts.\n\n\ngulp lint:test\n: Lint only the test scripts.\n\n\ngulp lint:scripts\n: Lint only the project source scripts.\n\n\n--fix\n: Add this flag (like \ngulp lint --fix\n or \ngulp lint:build --fix\n)\n   to auto-fix some errors, where ESLint has support to do so.", 
            "title": "Testing"
        }, 
        {
            "location": "/testing-fe/#browser-tests", 
            "text": "", 
            "title": "Browser tests"
        }, 
        {
            "location": "/testing-fe/#quick-start", 
            "text": "To run browser tests, open a new Terminal window or tab and change to the project directory,\nthen tell gulp to start the tests:  gulp build\ngulp test:acceptance ( tox -e acceptance can be run as well )  There are several options you can pass to run a particular suite of tests,\nto run a particular list of features,\nand/or to run it in \"fast\" mode:  gulp test:acceptance --suite=wagtail-admin ( runs just the wagtail-admin suite )\ngulp test:acceptance --specs=multi-select.feature ( runs just the multi-select feature )\ngulp test:acceptance --tags=@mobile ( runs all scenarios tagged with @mobile )\ngulp test:acceptance --fast ( runs the tests without recreating the virtual environment )  The same options can be used with tox (--omitted):  tox -e acceptance suite=wagtail-admin\ntox -e acceptance specs=multi-select.feature\ntox -e acceptance tags=@mobile\ntox -e acceptance-fast  These tests will run on their own server; you do not need to be running your development server.", 
            "title": "Quick start:"
        }, 
        {
            "location": "/testing-fe/#cucumber-tool-for-running-automated-tests-written-in-plain-language", 
            "text": "Below are some suggested standards for Cucumber Feature files:  Table copied from  https://saucelabs.com/blog/write-great-cucumber-tests  by Greg Sypolt, with moderate modifications  \n    \n       \n          Feature Files \n          Every *.feature file consists in a single feature, focused on the business value. \n       \n       \n       Gherkin \n          \n             Feature:Title (one line describing the story)\nNarrative Description: As a [role], I want [feature], so that I [benefit] \nScenario: Title (acceptance criteria of user story)\n  Given [context]\n  And [some more context]...\n  When [event]\n  Then [outcome]\n  And [another outcome]... \nScenario:...  \n       \n       \n          Given, When, and Then Statements \n          \n           There might be some confusion surrounding where to put the verification step in the Given, When, Then sequence. Each statement has a purpose.    Given  is the pre-condition to put the system into a known state before the user starts interacting with the application  When  describes the key action the user performs  Then  is observing the expected outcome   Just remember the  \u2018then\u2019  step is an acceptance criteria of the story.\n    \n       \n       \n          Background \n          The background needs to be used wisely. If you use the same steps at the beginning of all scenarios of a feature, put them into the feature\u2019s background scenario. The background steps are run before each scenario. \nBackground:\n  Given I am logged into Wagtail as an admin\n  And I create a Wagtail Sublanding Page\n  And I open the content menu \n         \n       \n       \n          Scenarios \n          Keep each scenario independent. The scenarios should run independently, without any dependencies on other scenarios.  Scenarios should be between 3 to 6 statements, if possible. \n       \n       \n          Scenario Outline \n          If you identify the need to use a scenario outline, take a step back and ask the following question: Is it necessary to repeat this scenario \u2018x\u2019 amount of times just to exercise the different combination of data? In most cases, one time is enough for UI level testing. \n       \n       \n          Declarative Vs Imperative Scenarios \n          \n            The declarative style describes behavior at a higher level, which improves the readability of the feature by abstracting out the implementation details of the application.  The imperative style is more verbose but better describes the expected behavior.  Either style is acceptable.  Example: Declarative  \nScenario:User logs in\n  Given I am on the homepage\n  When I log in\n  Then I should see a login notification  Example: Imperative  \nScenario: User logs in\n  Given I am on the homepage\n  When I click on the \"Login\" button\n  And I fill in the \"Email\" field with \" \"\n  And I fill in the \"Password\" field with \"secret\"\n  And I click on \"Submit\"\n  Then I should see \"Welcome to the app, John Doe\"", 
            "title": "Cucumber - tool for running automated tests written in plain language"
        }, 
        {
            "location": "/testing-fe/#sauce-connect-send-tests-to-the-cloud", 
            "text": "Sauce Labs can be used to run tests remotely in the cloud.    Log into  http://saucelabs.com/account .    Update and uncomment the  SAUCE_USERNAME ,  SAUCE_ACCESS_KEY ,\n   and  SAUCE_SELENIUM_URL  values in your  .env  file.\n   The access key can be found on the Sauce Labs\n    user settings page .    Reload the settings with  source .env .    Run the tests with  gulp test:acceptance --sauce .    Monitor progress of the tests\n   on the  Sauce Labs dashboard  Automated Tests tab.     Note  If you get the error  Error: ENOTFOUND getaddrinfo ENOTFOUND \nwhile running a test, it likely means that Sauce Connect is not running.", 
            "title": "Sauce Connect - send tests to the cloud"
        }, 
        {
            "location": "/testing-fe/#manual-test-configuration", 
            "text": "A number of command-line arguments can be set to test particular configurations:   --suite : Choose a particular suite or suites to run.\n   For example,  gulp test:acceptance --suite=content  or  gulp test:acceptance --suite=content,functional .  --specs : Choose a particular spec or specs to run.\n   For example,  gulp test:acceptance --specs=header.feature ,  gulp test:acceptance --specs=header.feature,pagination.feature , or  gulp test:acceptance --specs=filterable*.feature . If  --suite  is specified, this argument will be ignored. If neither  --suite  nor  --specs  are specified, all specs will be run.  --windowSize : Set the window size in pixels in  w,h  format.\n   For example,  gulp test:acceptance --windowSize=900,400 .  --browserName : Set the browser to run.\n   For example,  gulp test:acceptance --browserName=firefox .  --version : Set the browser version to run.\n   For example,  gulp test:acceptance --version='44.0' .  --platform : Set the OS platform to run.\n   For example,  gulp test:acceptance --platform='osx 10.10' .  --sauce : Whether to run on Sauce Labs or not.\n   For example,  gulp test:acceptance --sauce=false .", 
            "title": "Manual test configuration"
        }, 
        {
            "location": "/testing-fe/#tests", 
            "text": "Tests are organized into suites under the  test/browser_tests/cucumber/features  directory. Any new tests should be added to an existing suite (e.g. \"default\"), or placed into a new suite directory. All tests start with writing a  .feature  spec in one of these suites, and then adding corresponding step definitions, found in  test/browser_tests/cucumber/step_definitions .", 
            "title": "Tests"
        }, 
        {
            "location": "/testing-fe/#further-reading", 
            "text": "Cucumber features  Protractor  Select elements on a page  Writing Jasmin expectations .  Understanding Page Objects", 
            "title": "Further reading"
        }, 
        {
            "location": "/testing-fe/#performance-testing", 
            "text": "To audit if the site complies with performance best practices and guidelines,\nrun  gulp audit:perf .  The audit will run against Google's PageSpeed Insights .", 
            "title": "Performance testing"
        }, 
        {
            "location": "/testing-fe/#unit-testing", 
            "text": "", 
            "title": "Unit testing"
        }, 
        {
            "location": "/testing-fe/#javascript-unit-tests", 
            "text": "JavaScript module unit tests are run with  gulp test:unit .  If you want to run individual spec files, pass in the  --specs  command-line\nargument with the path to the spec,\nsuch as  gulp test:unit --specs=modules/Tree-spec.js .\nGlobs can be used to run a group of unit tests in the same directory,\nsuch as  gulp test:unit --specs=modules/transition/*.js .", 
            "title": "JavaScript unit tests"
        }, 
        {
            "location": "/testing-fe/#accessibility-testing", 
            "text": "Run the acceptance tests with an  --a11y  flag (i.e.  gulp test:acceptance --a11y )\nto check every webpage for WCAG and Section 508 compliancy using Protractor's accessibility plugin .  If you'd like to audit a specific page, use  gulp audit:a11y :   Enable the environment variable  ACHECKER_ID  in your  .env  file.\n     Get a free  AChecker API ID  for the value.  Reload your  .env  with  source ./.env  while in the project root directory.  Run  gulp test:a11y  to run an audit on the homepage.  To test a page aside from the homepage, add the  --u= path_to_test  flag.\n     For example,  gulp test:a11y --u=contact-us \n     or  gulp test:a11y --u=the-bureau/bureau-structure/ .", 
            "title": "Accessibility Testing"
        }, 
        {
            "location": "/testing-fe/#source-code-linting", 
            "text": "The default test task includes linting of the JavaScript source, build,\nand test files.\nUse the  gulp lint  command from the command-line to run the ESLint linter,\nwhich checks the JavaScript against the rules configured in  .eslintrc . See the ESLint docs \nfor detailed rule descriptions.  There are a number of options to the command:   gulp lint:build : Lint only the gulp build scripts.  gulp lint:test : Lint only the test scripts.  gulp lint:scripts : Lint only the project source scripts.  --fix : Add this flag (like  gulp lint --fix  or  gulp lint:build --fix )\n   to auto-fix some errors, where ESLint has support to do so.", 
            "title": "Source code linting"
        }, 
        {
            "location": "/development-tips/", 
            "text": "Development tips\n\n\nTIP: Loading sibling projects\n\n\nSome projects fit within the cfgov-refresh architecture,\nbut are not fully incorporated into the project.\nThese are known as \"non-v1 Django apps.\"\nIn order to visit areas of the site locally where those projects are used,\nthe sibling projects need to be installed\nand then indexed within this cfgov-refresh project.\n\n\nThe non-v1 apps are the following:\n\n\n\n\nOwning a Home\n.\n\n\nfin-ed-resources (ghe/CFGOV/fin-ed-resources) - for the Education Resources section.\n\n\nknow-before-you-owe (ghe/CFGOV/know-before-you-owe) - for the Consumer Tools \n Know before you owe section.\n\n\n\n\nAfter installing these projects as sibling directories to the \ncfgov-refresh\n repository,\n\n\nOption 1: Sheer Index and Elasticsearch (e.g. owning-a-home)\n\n\n\n\nbuild the third-party projects per their directions,\n\n\nstop the web server and return to \ncfgov-refresh\n\n\nand run \ncfgov/manage.py sheer_index -r\n to load the projects' data into ElasticSearch.\n\n\n\n\nOption 2: Direct dependencies\n\n\n\n\nBuild the third-party projects per their directions\n\n\nStop the web server and return to \ncfgov-refresh\n\n\nRun \npip install -e ../\nsibling\n to load the projects' dependencies\n\n\n\n\n\n\nNote\n\n\nDo not install the projects directly into the \ncfgov-refresh\n directory.\nClone and install the projects as siblings to \ncfgov-refresh\n,\nso that they share the same parent directory (\n~/Projects\n or similar).\n\n\n\n\nTIP: Loading data into Django models\n\n\nThe Django management command \nimport-data\n will import data from the specified\nsource into the specified model.\n\n\nusage: manage.py import-data [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS]\n        [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--parent PARENT]\n        [--snippet] -u USERNAME -p PASSWORD [--app APP] [--overwrite]\n        data_type wagtail_type\n\n- \ndata_type\n is the WordPress post type defined in the \nprocessors.py\n file.\n- \nwagtail_type\n is the Django model name where the data is going to go.\n- \n-u\n and \n-p\n are credentials to an admin account.\n\n\nRequired option:\n\n\n\n\n--parent\n is the slug of the parent page that the pages will exist\n  under.\n\n\n--snippet\n is a flag that's used to signify that the importing data will be\n  inserted into a Django model, registered as a \nWagtail snippet\n.\n  One of these options must be set for the command to run.\n\n\n\n\nOther options:\n\n\n\n\n--app\n is the name of the app the Django models from \nwagtail_type\n exist in.\n  It defaults to our app, \nv1\n.\n\n\n--overwrite\n overwrites existing pages in Wagtail based on comparing slugs.\nBe careful when using this as it will overwrite the data in Wagtail with data\nfrom the source. Default is \nFalse\n.\n\n\n--verbosity\n is set to 1 by default. Set it to 2 or higher and expect the\nname of the slugs to appear where appropriate.\n\n\n\n\nFor now, in order for this command to import the data, one of the things it\nneeds is a file for \"sheer logic\" to use to retrieve the data. For us, the\nprocessors are already done from our last backend. This part of the command\nwill change as we move away from our dependency on \"sheer logic.\" This is set\nby putting the file in a \nprocessors\n module in the top level of the project\nand adding it to the setting \nSHEER_PROCESSORS\n.\n\n\nThe command needs a \nprocessors\n module in the app that's passed to it, as well\nas a file with the same name as the Django model specified that defines a class\nnamed \nDataConverter\n that subclasses either \n_helpers.PageDataConverter\n or\n\n_helpers.SnippetDataConverter\n and implements their method(s) explained below:\n\n\nPageDataConverter\n:\n\n\n\n\nconvert(self, imported_data)\n:\n   For converting pages or snippets, the processor file must implement the\n   \nconvert()\n function with one argument. That argument represents the\n   imported data dictionary. That function must take the dictionary and map it\n   to a new one that uses the keys that Wagtail's \ncreate()\n and \nedit()\n\n   admin/snippet view functions expect in the \nrequest.POST\n dictionary to\n   actually migrate the data over, and then returns that dictionary where it\n   will be assigned to \nrequest.POST\n.\n\n\n\n\nSnippetDataConverter(PageDataConverter)\n:\n\n\n\n\nget_existing_snippet()\n\n   This also accepts the imported data dictionary. It's used to find an\n   existing snippet given the imported data, returning it if found or \nNone\n if\n   not.\n\n\n\n\nTIP: Working with the templates\n\n\nFront-End Template/Asset Locations\n\n\nTemplates\n that are served by the Django server: \ncfgov\\jinja2\\v1\n\n\nStatic assets\n prior to processing (minifying etc.): \ncfgov\\unprocessed\n.\n\n\n\n\nNote\n\n\nAfter running \ngulp build\n the site's assets are copied over to \ncfgov\\static_built\n,\nready to be served by Django.\n\n\n\n\nSimple static template setup\n\n\nBy default, Django will render pages with accordance to the URL pattern defined\nfor it. For example, going to \nhttp://localhost:8000/the-bureau/index.html\n\n(or \nhttp://localhost:8000/the-bureau/\n) renders \n/the-bureau/index.html\n from\nthe \ncfgov\n app folder's \njinja2/v1\n templates folder as processed\nby the \nJinja2\n templating engine.\n\n\nTIP: Outputting indexed content in a Sheer template\n\n\nMost of our content is indexed from the API output of our WordPress back-end.\nThis happens when the \npython cfgov/manage.py sheer_index -r\n command is run.\n\n\nThere are two ways in which we use indexed content:\nrepeating items (e.g., blog posts and press releases),\nand single pages (e.g., the Future Requests page in Doing Business with Us).\nWhat follows is a deeper dive into both of these content types.\n\n\nSingle content\n\n\nTo access a single piece of content,\nthe easiest thing to do is use the \nget_document()\n function.\n\n\nUsing the example given earlier of the Future Requests page,\nhere's how it's done:\n\n\n{% set page = get_document('pages', '63169') %}\n{{ page.content | safe }}\n\n\n\nThe \nget_document\n method can be used to retrieve a single item of any post type\nfor display within a template.\nIn the below example we get an instance of the non-hierarchical\n\ncontact\n post type using its slug (\nwhistleblowers\n):\n\n\n{% set whistleblowers = get_document('contact', 'whistleblowers') %}\n\n\n\nIn practice, many of our templates are a Frankenstein-type mixture\nof hand-coded static content and calls to indexed content,\nas we continually try to strike the right balance of what content\nis appropriate to be edited by non-developers in Wagtail,\nand what is just too fragile to do any other way than by hand.\n\n\nTIP: Filtering results with queries\n\n\nSometimes you'll want to create queries in your templates to filter the data.\n\n\nThe two main ways of injecting filters into your data are in the URL's query\nstring and within the template code itself.\n\n\nWe have a handy function \nsearch()\n that:\n\n\n\n\nPulls in filters from the URL query string.\n\n\nAllows you to add additional filters by passing them in as arguments to the function.\n\n\n\n\nURL query string filters\n\n\nURL query string filters can be further broken down into two types:\n\n\n\n\nTerm - Used when you want to filter by whether a field matches a term.\nNote that in order to use this type of filter,\nthe field you are matching it against must have\n\n\"index\": \"not_analyzed\"\n set in the mapping.\n\n\nRange - Used for when you want to filter something by a range (e.g. dates or numbers)\n\n\n\n\nAn example of Term is:\n\n\n?filter_category=Op-Ed\n\n\nfilter_[field]=[value]\n\n\nAn example of Range is:\n\n\n?filter_range_date_gte=2014-01\n\n\nfilter_range_[field]_[operator]=[value]\n\n\nURL query string filters are convenient for many of the filtered queries you'll need to run,\nbut often there are cases where you'll need more flexibility.\n\n\nMore complex filters\n\n\nBy default, \nsearch()\n uses the default query parameters\ndefined in the \n_queries/object-name.json\n file,\nthen mixes them in with any additional arguments\nfrom the URL query string in addition to what is passed into the function itself.\n\n\nWhen using \nsearch()\n, you can also pass in filters with the same \nfilter_\n syntax as above.\n\n\nFor example:\n\n\nsearch(filter_category='Op-Ed')\n\n\nMultiple term filters on the same field will be combined in an OR clause, while\nterm filters of different fields will be combined in an AND clause.\n\n\nFor example:\n\n\nsearch(filter_tag='Students', filter_tag='Finance', filter_author='Batman')\n\n\nThis will return documents that have the tag Students OR Finance, AND have an author of Batman.\n\n\nIf you need more control over your filter than that,\nenter it manually in the \ncfgov/jinja2/v1/_queries/[filtername].json\n file.\n\n\nTIP: Debugging site performance\n\n\nWhen running locally it is possible to enable the\n\nDjango Debug Toolbar\n\nby defining the \nENABLE_DEBUG_TOOLBAR\n environment variable:\n\n\n$ ENABLE_DEBUG_TOOLBAR=1 ./runserver.sh\n\n\n\nThis tool exposes various useful pieces of information about things like HTTP headers,\nDjango settings, SQL queries, and template variables. Note that running with the toolbar on\nmay have an impact on local server performance.\n\n\nTIP: Updating the documentation\n\n\nOur documentation is written as Markdown files and served in GitHub pages\nby \nmkdocs\n.\n\n\nTo update the docs in GitHub Pages once a pull request has been merged,\nmkdocs provides \na helpful command\n:\n\n\nmkdocs gh-deploy --clean", 
            "title": "Development tips"
        }, 
        {
            "location": "/development-tips/#development-tips", 
            "text": "", 
            "title": "Development tips"
        }, 
        {
            "location": "/development-tips/#tip-loading-sibling-projects", 
            "text": "Some projects fit within the cfgov-refresh architecture,\nbut are not fully incorporated into the project.\nThese are known as \"non-v1 Django apps.\"\nIn order to visit areas of the site locally where those projects are used,\nthe sibling projects need to be installed\nand then indexed within this cfgov-refresh project.  The non-v1 apps are the following:   Owning a Home .  fin-ed-resources (ghe/CFGOV/fin-ed-resources) - for the Education Resources section.  know-before-you-owe (ghe/CFGOV/know-before-you-owe) - for the Consumer Tools   Know before you owe section.   After installing these projects as sibling directories to the  cfgov-refresh  repository,", 
            "title": "TIP: Loading sibling projects"
        }, 
        {
            "location": "/development-tips/#option-1-sheer-index-and-elasticsearch-eg-owning-a-home", 
            "text": "build the third-party projects per their directions,  stop the web server and return to  cfgov-refresh  and run  cfgov/manage.py sheer_index -r  to load the projects' data into ElasticSearch.", 
            "title": "Option 1: Sheer Index and Elasticsearch (e.g. owning-a-home)"
        }, 
        {
            "location": "/development-tips/#option-2-direct-dependencies", 
            "text": "Build the third-party projects per their directions  Stop the web server and return to  cfgov-refresh  Run  pip install -e ../ sibling  to load the projects' dependencies    Note  Do not install the projects directly into the  cfgov-refresh  directory.\nClone and install the projects as siblings to  cfgov-refresh ,\nso that they share the same parent directory ( ~/Projects  or similar).", 
            "title": "Option 2: Direct dependencies"
        }, 
        {
            "location": "/development-tips/#tip-loading-data-into-django-models", 
            "text": "The Django management command  import-data  will import data from the specified\nsource into the specified model.  usage: manage.py import-data [-h] [--version] [-v {0,1,2,3}] [--settings SETTINGS]\n        [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--parent PARENT]\n        [--snippet] -u USERNAME -p PASSWORD [--app APP] [--overwrite]\n        data_type wagtail_type \n-  data_type  is the WordPress post type defined in the  processors.py  file.\n-  wagtail_type  is the Django model name where the data is going to go.\n-  -u  and  -p  are credentials to an admin account.  Required option:   --parent  is the slug of the parent page that the pages will exist\n  under.  --snippet  is a flag that's used to signify that the importing data will be\n  inserted into a Django model, registered as a  Wagtail snippet .\n  One of these options must be set for the command to run.   Other options:   --app  is the name of the app the Django models from  wagtail_type  exist in.\n  It defaults to our app,  v1 .  --overwrite  overwrites existing pages in Wagtail based on comparing slugs.\nBe careful when using this as it will overwrite the data in Wagtail with data\nfrom the source. Default is  False .  --verbosity  is set to 1 by default. Set it to 2 or higher and expect the\nname of the slugs to appear where appropriate.   For now, in order for this command to import the data, one of the things it\nneeds is a file for \"sheer logic\" to use to retrieve the data. For us, the\nprocessors are already done from our last backend. This part of the command\nwill change as we move away from our dependency on \"sheer logic.\" This is set\nby putting the file in a  processors  module in the top level of the project\nand adding it to the setting  SHEER_PROCESSORS .  The command needs a  processors  module in the app that's passed to it, as well\nas a file with the same name as the Django model specified that defines a class\nnamed  DataConverter  that subclasses either  _helpers.PageDataConverter  or _helpers.SnippetDataConverter  and implements their method(s) explained below:  PageDataConverter :   convert(self, imported_data) :\n   For converting pages or snippets, the processor file must implement the\n    convert()  function with one argument. That argument represents the\n   imported data dictionary. That function must take the dictionary and map it\n   to a new one that uses the keys that Wagtail's  create()  and  edit() \n   admin/snippet view functions expect in the  request.POST  dictionary to\n   actually migrate the data over, and then returns that dictionary where it\n   will be assigned to  request.POST .   SnippetDataConverter(PageDataConverter) :   get_existing_snippet() \n   This also accepts the imported data dictionary. It's used to find an\n   existing snippet given the imported data, returning it if found or  None  if\n   not.", 
            "title": "TIP: Loading data into Django models"
        }, 
        {
            "location": "/development-tips/#tip-working-with-the-templates", 
            "text": "", 
            "title": "TIP: Working with the templates"
        }, 
        {
            "location": "/development-tips/#front-end-templateasset-locations", 
            "text": "Templates  that are served by the Django server:  cfgov\\jinja2\\v1  Static assets  prior to processing (minifying etc.):  cfgov\\unprocessed .   Note  After running  gulp build  the site's assets are copied over to  cfgov\\static_built ,\nready to be served by Django.", 
            "title": "Front-End Template/Asset Locations"
        }, 
        {
            "location": "/development-tips/#simple-static-template-setup", 
            "text": "By default, Django will render pages with accordance to the URL pattern defined\nfor it. For example, going to  http://localhost:8000/the-bureau/index.html \n(or  http://localhost:8000/the-bureau/ ) renders  /the-bureau/index.html  from\nthe  cfgov  app folder's  jinja2/v1  templates folder as processed\nby the  Jinja2  templating engine.", 
            "title": "Simple static template setup"
        }, 
        {
            "location": "/development-tips/#tip-outputting-indexed-content-in-a-sheer-template", 
            "text": "Most of our content is indexed from the API output of our WordPress back-end.\nThis happens when the  python cfgov/manage.py sheer_index -r  command is run.  There are two ways in which we use indexed content:\nrepeating items (e.g., blog posts and press releases),\nand single pages (e.g., the Future Requests page in Doing Business with Us).\nWhat follows is a deeper dive into both of these content types.", 
            "title": "TIP: Outputting indexed content in a Sheer template"
        }, 
        {
            "location": "/development-tips/#single-content", 
            "text": "To access a single piece of content,\nthe easiest thing to do is use the  get_document()  function.  Using the example given earlier of the Future Requests page,\nhere's how it's done:  {% set page = get_document('pages', '63169') %}\n{{ page.content | safe }}  The  get_document  method can be used to retrieve a single item of any post type\nfor display within a template.\nIn the below example we get an instance of the non-hierarchical contact  post type using its slug ( whistleblowers ):  {% set whistleblowers = get_document('contact', 'whistleblowers') %}  In practice, many of our templates are a Frankenstein-type mixture\nof hand-coded static content and calls to indexed content,\nas we continually try to strike the right balance of what content\nis appropriate to be edited by non-developers in Wagtail,\nand what is just too fragile to do any other way than by hand.", 
            "title": "Single content"
        }, 
        {
            "location": "/development-tips/#tip-filtering-results-with-queries", 
            "text": "Sometimes you'll want to create queries in your templates to filter the data.  The two main ways of injecting filters into your data are in the URL's query\nstring and within the template code itself.  We have a handy function  search()  that:   Pulls in filters from the URL query string.  Allows you to add additional filters by passing them in as arguments to the function.", 
            "title": "TIP: Filtering results with queries"
        }, 
        {
            "location": "/development-tips/#url-query-string-filters", 
            "text": "URL query string filters can be further broken down into two types:   Term - Used when you want to filter by whether a field matches a term.\nNote that in order to use this type of filter,\nthe field you are matching it against must have \"index\": \"not_analyzed\"  set in the mapping.  Range - Used for when you want to filter something by a range (e.g. dates or numbers)   An example of Term is:  ?filter_category=Op-Ed  filter_[field]=[value]  An example of Range is:  ?filter_range_date_gte=2014-01  filter_range_[field]_[operator]=[value]  URL query string filters are convenient for many of the filtered queries you'll need to run,\nbut often there are cases where you'll need more flexibility.", 
            "title": "URL query string filters"
        }, 
        {
            "location": "/development-tips/#more-complex-filters", 
            "text": "By default,  search()  uses the default query parameters\ndefined in the  _queries/object-name.json  file,\nthen mixes them in with any additional arguments\nfrom the URL query string in addition to what is passed into the function itself.  When using  search() , you can also pass in filters with the same  filter_  syntax as above.  For example:  search(filter_category='Op-Ed')  Multiple term filters on the same field will be combined in an OR clause, while\nterm filters of different fields will be combined in an AND clause.  For example:  search(filter_tag='Students', filter_tag='Finance', filter_author='Batman')  This will return documents that have the tag Students OR Finance, AND have an author of Batman.  If you need more control over your filter than that,\nenter it manually in the  cfgov/jinja2/v1/_queries/[filtername].json  file.", 
            "title": "More complex filters"
        }, 
        {
            "location": "/development-tips/#tip-debugging-site-performance", 
            "text": "When running locally it is possible to enable the Django Debug Toolbar \nby defining the  ENABLE_DEBUG_TOOLBAR  environment variable:  $ ENABLE_DEBUG_TOOLBAR=1 ./runserver.sh  This tool exposes various useful pieces of information about things like HTTP headers,\nDjango settings, SQL queries, and template variables. Note that running with the toolbar on\nmay have an impact on local server performance.", 
            "title": "TIP: Debugging site performance"
        }, 
        {
            "location": "/development-tips/#tip-updating-the-documentation", 
            "text": "Our documentation is written as Markdown files and served in GitHub pages\nby  mkdocs .  To update the docs in GitHub Pages once a pull request has been merged,\nmkdocs provides  a helpful command :  mkdocs gh-deploy --clean", 
            "title": "TIP: Updating the documentation"
        }, 
        {
            "location": "/ask-cfpb/", 
            "text": "Ask CFPB API\n\n\nThis API provides search access to the English and Spanish content behind \nAsk CFPB\n and \nObtener respuestas\n.\n\n\nThe financial topics covered include:\n\n\n\n\nAuto loans\n\n\nBank accounts and services\n\n\nCredit cards\n\n\nCredit reports and scores\n\n\nDebt collection\n\n\nFamilies and money\n\n\nMoney transfers\n\n\nMortgages\n\n\nPayday loans\n\n\nPrepaid cards\n\n\nStudent loans\n\n\n\n\nUsage\n\n\nThe API is a read-only resource that delivers search results in \njson\n format.\n\n\nRequests follow this pattern:\n\n\n\n\nhttps://www.consumerfinance.gov/ask-cfpb/search/json/?q=[SEARCH TERMS]\n\n\n\n\nThe json response will includes a list of results, each with a question, an answer, and a URL for the related CFPB page. \nIf no results are found, the \"suggestion\" field will offer a more promising search term if one can be found.\n\n\nThe payload for the search term \"tuition\" would look like this, but with more result entries:\n\n\n{\n  query: \ntuition\n,\n  suggestion: null,\n  result_query: \ntuition\n,\n  results: [\n    {\n      url: \nhttp://www.consumerfinance.gov/ask-cfpb/what-is-a-tuition-payment-plan-en-563/ \n,\n      text: \nTuition payment plans, also called tuition installment plans, are short-term (12 months or less) payment plans that split your college bills into equal monthly payments. Tuition installment plans can be an alternative to student loans if you can afford to pay tuition, just not in a lump sum at the start of the semester or quarter. These payment plans do not generally charge interest, but they may have up-front fees. What is a tuition payment plan? \n,\n      question: \nWhat is a tuition payment plan? \n\n    }\n  ]\n}\n\n\n\nSpanish content\n\n\nFor questions and answers in Spanish, requests should follow this pattern:\n\n\n\n\nhttps://www.consumerfinance.gov/es/obtener-respuestas/buscar/json/?q=[SPANISH SEARCH TERMS]\n\n\n\n\nThe payload for the Spanish search term \"vehiculo\" would look like this, but with more result entries:\n\n\n{\n  query: \nvehiculo\n,\n  suggestion: null,\n  result_query: \nvehiculo\n,\n  results: [\n    {\n      url: \nhttp://www.consumerfinance.gov/es/obtener-respuestas/como-puedo-averiguar-el-significado-de-los-terminos-de-mi-contrato-de-leasing-es-2047/ \n,\n      text: \n Bajo la Ley de Arrendamientos del Consumidor (CLA, por sus siglas en ingl\u00e9s), la persona o compa\u00f1\u00eda de quien usted hace el leasing de un veh\u00edculo, conocida como el \narrendador\n, deber\u00e1 informar por escrito ciertos costos y plazos si el leasing es de m\u00e1s de cuatro meses y si cumple con otros requisitos. La mayor\u00eda de los arrendamientos de veh\u00edculos est\u00e1 sujeta a la CLA. Los siguientes materiales le pueden ayudar a entender los t\u00e9rminos de su contrato de leasing. En el sitio web Comprenda c\u00f3mo funciona la financiaci\u00f3n de veh\u00edculos de la Comisi\u00f3n Federal de Comercio se ofrece la siguiente informaci\u00f3n en espa\u00f1ol: Antes de comprar un veh\u00edculo o hacer un leasing \u00bfDeber\u00eda hacer un leasing para un veh\u00edculo? Glosario de t\u00e9rminos espec\u00edficos M\u00e1s informaci\u00f3n en espa\u00f1ol de GobiernoUSA.gov: Consejos para comprar un auto usado: Arrendamiento con derecho a compra o \u201cleasing\u201d     Bajo la Ley de Arrendamientos del Consumidor (CLA, por sus siglas en ingles), la persona o compania de quien usted hace el leasing de un vehiculo, conocida como el \narrendador\n, debera informar por escrito ciertos costos y plazos si el leasing es de mas de cuatro meses y si cumple con otros requisitos. La mayoria de los arrendamientos de vehiculos esta sujeta a la CLA. Los siguientes materiales le pueden ayudar a entender los terminos de su contrato de leasing. En el sitio web Comprenda como funciona la financiacion de vehiculos de la Comision Federal de Comercio se ofrece la siguiente informacion en espanol: Antes de comprar un vehiculo o hacer un leasing Deberia hacer un leasing para un vehiculo? Glosario de terminos especificos Mas informacion en espanol de GobiernoUSA.gov: Consejos para comprar un auto usado: Arrendamiento con derecho a compra o leasing \u00bfC\u00f3mo puedo averiguar el significado de los t\u00e9rminos de mi contrato de leasing? Como puedo averiguar el significado de los terminos de mi contrato de leasing? \n,\n      question: \n\u00bfC\u00f3mo puedo averiguar el significado de los t\u00e9rminos de mi contrato de leasing? \n\n    }\n  ]\n}", 
            "title": "Ask CFPB"
        }, 
        {
            "location": "/ask-cfpb/#ask-cfpb-api", 
            "text": "This API provides search access to the English and Spanish content behind  Ask CFPB  and  Obtener respuestas .  The financial topics covered include:   Auto loans  Bank accounts and services  Credit cards  Credit reports and scores  Debt collection  Families and money  Money transfers  Mortgages  Payday loans  Prepaid cards  Student loans", 
            "title": "Ask CFPB API"
        }, 
        {
            "location": "/ask-cfpb/#usage", 
            "text": "The API is a read-only resource that delivers search results in  json  format.  Requests follow this pattern:   https://www.consumerfinance.gov/ask-cfpb/search/json/?q=[SEARCH TERMS]   The json response will includes a list of results, each with a question, an answer, and a URL for the related CFPB page. \nIf no results are found, the \"suggestion\" field will offer a more promising search term if one can be found.  The payload for the search term \"tuition\" would look like this, but with more result entries:  {\n  query:  tuition ,\n  suggestion: null,\n  result_query:  tuition ,\n  results: [\n    {\n      url:  http://www.consumerfinance.gov/ask-cfpb/what-is-a-tuition-payment-plan-en-563/  ,\n      text:  Tuition payment plans, also called tuition installment plans, are short-term (12 months or less) payment plans that split your college bills into equal monthly payments. Tuition installment plans can be an alternative to student loans if you can afford to pay tuition, just not in a lump sum at the start of the semester or quarter. These payment plans do not generally charge interest, but they may have up-front fees. What is a tuition payment plan?  ,\n      question:  What is a tuition payment plan?  \n    }\n  ]\n}", 
            "title": "Usage"
        }, 
        {
            "location": "/ask-cfpb/#spanish-content", 
            "text": "For questions and answers in Spanish, requests should follow this pattern:   https://www.consumerfinance.gov/es/obtener-respuestas/buscar/json/?q=[SPANISH SEARCH TERMS]   The payload for the Spanish search term \"vehiculo\" would look like this, but with more result entries:  {\n  query:  vehiculo ,\n  suggestion: null,\n  result_query:  vehiculo ,\n  results: [\n    {\n      url:  http://www.consumerfinance.gov/es/obtener-respuestas/como-puedo-averiguar-el-significado-de-los-terminos-de-mi-contrato-de-leasing-es-2047/  ,\n      text:   Bajo la Ley de Arrendamientos del Consumidor (CLA, por sus siglas en ingl\u00e9s), la persona o compa\u00f1\u00eda de quien usted hace el leasing de un veh\u00edculo, conocida como el  arrendador , deber\u00e1 informar por escrito ciertos costos y plazos si el leasing es de m\u00e1s de cuatro meses y si cumple con otros requisitos. La mayor\u00eda de los arrendamientos de veh\u00edculos est\u00e1 sujeta a la CLA. Los siguientes materiales le pueden ayudar a entender los t\u00e9rminos de su contrato de leasing. En el sitio web Comprenda c\u00f3mo funciona la financiaci\u00f3n de veh\u00edculos de la Comisi\u00f3n Federal de Comercio se ofrece la siguiente informaci\u00f3n en espa\u00f1ol: Antes de comprar un veh\u00edculo o hacer un leasing \u00bfDeber\u00eda hacer un leasing para un veh\u00edculo? Glosario de t\u00e9rminos espec\u00edficos M\u00e1s informaci\u00f3n en espa\u00f1ol de GobiernoUSA.gov: Consejos para comprar un auto usado: Arrendamiento con derecho a compra o \u201cleasing\u201d     Bajo la Ley de Arrendamientos del Consumidor (CLA, por sus siglas en ingles), la persona o compania de quien usted hace el leasing de un vehiculo, conocida como el  arrendador , debera informar por escrito ciertos costos y plazos si el leasing es de mas de cuatro meses y si cumple con otros requisitos. La mayoria de los arrendamientos de vehiculos esta sujeta a la CLA. Los siguientes materiales le pueden ayudar a entender los terminos de su contrato de leasing. En el sitio web Comprenda como funciona la financiacion de vehiculos de la Comision Federal de Comercio se ofrece la siguiente informacion en espanol: Antes de comprar un vehiculo o hacer un leasing Deberia hacer un leasing para un vehiculo? Glosario de terminos especificos Mas informacion en espanol de GobiernoUSA.gov: Consejos para comprar un auto usado: Arrendamiento con derecho a compra o leasing \u00bfC\u00f3mo puedo averiguar el significado de los t\u00e9rminos de mi contrato de leasing? Como puedo averiguar el significado de los terminos de mi contrato de leasing?  ,\n      question:  \u00bfC\u00f3mo puedo averiguar el significado de los t\u00e9rminos de mi contrato de leasing?  \n    }\n  ]\n}", 
            "title": "Spanish content"
        }, 
        {
            "location": "/consumer-complaint-database/", 
            "text": "Consumer Complaint Database API\n\n\nA resource for searching complaints submitted to the CFPB\n\n\nEach week the CFPB sends thousands of consumers\u2019 complaints about financial products and services to companies for response. Those complaints are \npublished\n on our website after the company responds or after 15 days, whichever comes first.\n\n\nThe API allows automation of the same filtering and searching functions offered to website visitors.\n\n\nDetailed documentation for the search API can be found \nhere\n.\n\n\nNotes\n\n\nThe database generally updates daily, and contains certain information for each complaint, including the source of the complaint, the date of submission, and the company the complaint was sent to for response. The database also includes information about the actions taken by the company in response to the complaint, such as, whether the company\u2019s response was timely and how the company responded. If the consumer opts to share it and after we take steps to remove personal information, we publish the consumer\u2019s description of what happened. Companies also have the option to select a public response. Company level information should be considered in context of company size and/or market share. Complaints referred to other regulators, such as complaints about depository institutions with less than $10 billion in assets, are not published in the Consumer Complaint Database.", 
            "title": "Consumer complaints"
        }, 
        {
            "location": "/consumer-complaint-database/#consumer-complaint-database-api", 
            "text": "", 
            "title": "Consumer Complaint Database API"
        }, 
        {
            "location": "/consumer-complaint-database/#a-resource-for-searching-complaints-submitted-to-the-cfpb", 
            "text": "Each week the CFPB sends thousands of consumers\u2019 complaints about financial products and services to companies for response. Those complaints are  published  on our website after the company responds or after 15 days, whichever comes first.  The API allows automation of the same filtering and searching functions offered to website visitors.  Detailed documentation for the search API can be found  here .", 
            "title": "A resource for searching complaints submitted to the CFPB"
        }, 
        {
            "location": "/consumer-complaint-database/#notes", 
            "text": "The database generally updates daily, and contains certain information for each complaint, including the source of the complaint, the date of submission, and the company the complaint was sent to for response. The database also includes information about the actions taken by the company in response to the complaint, such as, whether the company\u2019s response was timely and how the company responded. If the consumer opts to share it and after we take steps to remove personal information, we publish the consumer\u2019s description of what happened. Companies also have the option to select a public response. Company level information should be considered in context of company size and/or market share. Complaints referred to other regulators, such as complaints about depository institutions with less than $10 billion in assets, are not published in the Consumer Complaint Database.", 
            "title": "Notes"
        }, 
        {
            "location": "/hmda/", 
            "text": "HMDA API docs\n\n\nA resource for data required by the Home Mortgage Disclosure Act\n\n\nThe CFPB \npublishes data\n gleaned from mortgage loan applications going back to 2007.\n\n\nThe API enables auotmated access to this rich data set. Full details on how to use it starts \nhere\n.", 
            "title": "HMDA data"
        }, 
        {
            "location": "/hmda/#hmda-api-docs", 
            "text": "", 
            "title": "HMDA API docs"
        }, 
        {
            "location": "/hmda/#a-resource-for-data-required-by-the-home-mortgage-disclosure-act", 
            "text": "The CFPB  publishes data  gleaned from mortgage loan applications going back to 2007.  The API enables auotmated access to this rich data set. Full details on how to use it starts  here .", 
            "title": "A resource for data required by the Home Mortgage Disclosure Act"
        }
    ]
}